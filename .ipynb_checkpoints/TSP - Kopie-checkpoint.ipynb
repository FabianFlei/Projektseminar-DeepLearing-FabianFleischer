{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1f3798",
   "metadata": {},
   "source": [
    "# Travelng Salesman Prblem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e10460",
   "metadata": {},
   "source": [
    "## Die Daten\n",
    "Der Link zu den Daten: <br>\n",
    "http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/ <br>\n",
    "<br>\n",
    "Die Dokumentation zu den Daten:<br>\n",
    "http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp95.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6a4af",
   "metadata": {},
   "source": [
    "Da die Daten nicht einfac dierekt lesbar sind habe ich eine Klasse geschrieben die die daten so aufbereitet das ich diese im meinem weiteren code einwandfrei einsetzen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc0710",
   "metadata": {},
   "source": [
    "### Formatierung\n",
    "Eine Zeile aus einem Datensatz sieht beispielsweise so aus: <br>\n",
    "'19 510.0 875.0  \\n'<br>\n",
    "Die 19 ist die Konten nummer, 510.0 der X-Wert und 875.0 der Y Wert<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfef8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([565, 575]), array([ 25, 185]), array([345, 750]), array([945, 685]), array([845, 655]), array([880, 660]), array([ 25, 230]), array([ 525, 1000]), array([ 580, 1175]), array([ 650, 1130]), array([1605,  620]), array([1220,  580]), array([1465,  200]), array([1530,    5]), array([845, 680]), array([725, 370]), array([145, 665]), array([415, 635]), array([510, 875]), array([560, 365]), array([300, 465]), array([520, 585]), array([480, 415]), array([835, 625]), array([975, 580]), array([1215,  245]), array([1320,  315]), array([1250,  400]), array([660, 180]), array([410, 250]), array([420, 555]), array([575, 665]), array([1150, 1160]), array([700, 580]), array([685, 595]), array([685, 610]), array([770, 610]), array([795, 645]), array([720, 635]), array([760, 650]), array([475, 960]), array([ 95, 260]), array([875, 920]), array([700, 500]), array([555, 815]), array([830, 485]), array([1170,   65]), array([830, 610]), array([605, 625]), array([595, 360]), array([1340,  725]), array([1740,  245])], {'name': 'berlin52', 'comment': '52 locations in Berlin (Groetschel)', 'type': 'TSP', 'dimension': 52, 'edge_weight_type': 'EUC_2D', 'node_coord_section': 'NODE_COORD_SECTION', 'maxValue': 1740})\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Als Grundlageninformation https://www.python-lernen.de/dateien-auslesen.htm\n",
    "class DataExtractor():\n",
    "    \n",
    "    def extraktData(dataSet = None):\n",
    "        metadata = {\"dataSet\": [\"a280.tsp\", \"brd14051.tsp\", \"berlin52.tsp\"]}\n",
    "\n",
    "        assert dataSet is None or dataSet in metadata[\"dataSet\"]\n",
    "\n",
    "\n",
    "        data = open(dataSet, 'r')\n",
    "\n",
    "        # Formatiere Name\n",
    "        name = str(data.readline())\n",
    "        find = name.find(\": \")\n",
    "        if find != -1:\n",
    "            name = name[find+2:len(name)-1]\n",
    "        else: \n",
    "            name = name[name.find(\":\")+1:len(name)-1]\n",
    "\n",
    "        # Formatiere Comment\n",
    "        comment = str(data.readline())\n",
    "        find = comment.find(\": \")\n",
    "        if find != -1:\n",
    "            comment = comment[find+2:len(comment)-1]\n",
    "        else: \n",
    "            comment = comment[comment.find(\":\")+1:len(comment)-1]\n",
    "        \n",
    "        # Formatiere Type\n",
    "        type = str(data.readline())\n",
    "        find = type.find(\": \")\n",
    "        if find != -1:\n",
    "            type = type[find+2:len(type)-1]\n",
    "        else: \n",
    "            type = type[type.find(\":\")+1:len(type)-1]\n",
    "\n",
    "        # Formatiere Dimension   \n",
    "        dimension = str(data.readline())\n",
    "        find = dimension.find(\": \")\n",
    "        if find != -1:\n",
    "            dimension = int(float(dimension[find+2:len(dimension)-1]))\n",
    "        else: \n",
    "            dimension = int(float(dimension[dimension.find(\":\")+1:len(dimension)-1]))\n",
    "\n",
    "        # Formatiere Edge_weight_type\n",
    "        edge_weight_type = str(data.readline())\n",
    "        find = edge_weight_type.find(\": \")\n",
    "        if find != -1:\n",
    "            edge_weight_type = edge_weight_type[find+2:len(edge_weight_type)-1]\n",
    "        else: \n",
    "            edge_weight_type = edge_weight_type[edge_weight_type.find(\":\")+1:len(edge_weight_type)-1]\n",
    "\n",
    "        # Formatiere Node_coord_section\n",
    "        node_coord_section = str(data.readline())\n",
    "        node_coord_section = node_coord_section[:len(node_coord_section)-1]\n",
    "\n",
    "        maxValue = 0\n",
    "\n",
    "        target_location = []\n",
    "\n",
    "        while True:\n",
    "            dataPoints = data.readline()\n",
    "            # Fall ist EOF erreicht == Alle Daten eingelesen\n",
    "            if dataPoints.find(\"EOF\") != -1:\n",
    "                break\n",
    "\n",
    "\n",
    "            index = int(str(dataPoints).index(\" \"))\n",
    "            while index == 0:\n",
    "                dataPoints = dataPoints[1:]\n",
    "                index = int(str(dataPoints).index(\" \"))\n",
    "\n",
    "            dataPoints = dataPoints[index:]\n",
    "            index = 0\n",
    "\n",
    "            while index == 0:\n",
    "                dataPoints = dataPoints[1:]\n",
    "                index = int(str(dataPoints).index(\" \"))\n",
    "\n",
    "            x = int(float(dataPoints[:int(str(dataPoints).index(\" \"))]))\n",
    "\n",
    "            if x > maxValue:\n",
    "                maxValue = x\n",
    "\n",
    "            dataPoints = dataPoints[index:]\n",
    "            find = 0\n",
    "\n",
    "            while find != -1 and find == 0:\n",
    "                dataPoints = dataPoints[1:]\n",
    "                find = int(str(dataPoints).find(\" \")) \n",
    "\n",
    "            if find != -1:\n",
    "                y = int(float(dataPoints[:len(dataPoints)-1])) # vor \\n am ende jeder Zeile ist noch ein Leerzeichen\n",
    "            else:\n",
    "                y = int(float(dataPoints[:len(dataPoints)-1])) # -2 da \\n am ende jeder Zeile\n",
    "\n",
    "            if y > maxValue:\n",
    "                maxValue = y\n",
    "\n",
    "\n",
    "            target_location.append(np.array([x,y]))\n",
    "\n",
    "        info = {\n",
    "                \"name\": name, \n",
    "                \"comment\": comment, \n",
    "                \"type\": type, \n",
    "                \"dimension\": dimension,\n",
    "                \"edge_weight_type\": edge_weight_type, \n",
    "                \"node_coord_section\": node_coord_section,\n",
    "                \"maxValue\": maxValue\n",
    "                }\n",
    "\n",
    "        return target_location, info\n",
    "\n",
    "print(DataExtractor.extraktData(dataSet=\"berlin52.tsp\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde1bcd",
   "metadata": {},
   "source": [
    "# Enviorment\n",
    "Für Reininforced Learning benötigt man einen Agent und ein Enviorment <br><br>\n",
    "Für das Enviorment nutze ich GYM und orientiere mich dabei an: <br>\n",
    "https://www.gymlibrary.dev/content/environment_creation/ <br><br>\n",
    "Änderungen die ich vornehmen muss: <br>\n",
    "-- Mehrere Ziele für den Agenten <br>\n",
    "Schwirigkeiten: <br>\n",
    "-- Alles in einen Jupiter Nodebook möglichmachen\n",
    "-- Observation space richtig zu definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc36c1a",
   "metadata": {},
   "source": [
    "### Requierments\n",
    "Folgendes muss installiert werden: <br>\n",
    "pip install pygame <br>\n",
    "pip install gym <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb38b51",
   "metadata": {},
   "source": [
    "#### Ordnerstruktur für die Enviorment Dateien um dieses zu Regiesteren damit es im Folgenden genutzt werden kann\n",
    "gym-Tsp/ <br> \n",
    "        README.md <br>\n",
    "        setup.py <br>\n",
    "        gym_TSP/ <br>\n",
    "            __init__.py <br>\n",
    "            envs/ <br>\n",
    "               __init__.py <br>\n",
    "               TSP_Env.py <br>\n",
    "               \n",
    "Daran gehalten https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d43336",
   "metadata": {},
   "source": [
    "#### Im folgenden wird gezeit was sich in welcher Datei befindet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d848bd2",
   "metadata": {},
   "source": [
    "gym-TSP\\README.md sollte eine kurze Beschreibung des Enviomrents enthalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c3181",
   "metadata": {},
   "source": [
    "gym-TSP\\setup.py der Name (name=\"gym_TSP\") den wir hier vergeben ist der Name der später bei import gym_TSP benutzt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e09d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-TSP\\setup.py\n",
    "\n",
    "#from setuptools import setup\n",
    "\n",
    "#setup(\n",
    "#    name=\"gym_TSP\",\n",
    "#    version=\"0.0.1\",\n",
    "#    install_requires=[\"gym==0.26.0\", \"pygame==2.1.0\"],\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3deed6d",
   "metadata": {},
   "source": [
    "gym-TSP\\gym_TSP\\__init__.py hier wird das Enviorment in gym registiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c75feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-TSP\\gym_TSP\\__init__.py\n",
    "\n",
    "#from gym.envs.registration import register\n",
    "\n",
    "#register(\n",
    "#    id='TSPEnv-v0', #wir in gym.make() für die Instatzierung aufgerufen \n",
    "#    entry_point='gym_TSP.envs:TSPEnv',\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c198d",
   "metadata": {},
   "source": [
    "gym-TSP\\gym_TSP\\envs\\__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb423f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-TSP\\gym_TSP\\envs\\__init__.py\n",
    "\n",
    "#from gym_TSP.envs.TSP_Env import TSPEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adf1c0",
   "metadata": {},
   "source": [
    "gym-TSP\\gym_TSP\\envs\\TSP_Env.py enthält das custom Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e40d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-TSP\\gym_TSP\\envs\\TSP_Env.py\n",
    "\n",
    "import gym # für das Enviorment zuständig\n",
    "from gym import spaces\n",
    "import pygame # für die Visualisierung zuständig und auführung der Actionen\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "class TSPEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    \n",
    "    def __init__(self, render_mode = None, size=5):\n",
    "        self.size = size #size of the square grid\n",
    "        self.window_size = 800 #size of the PyGame window\n",
    "\n",
    "        \"\"\"Hier änderung notwendig mehrfache Ziele\"\"\"\n",
    "        #Observations sind dictionaries mit der Positions des agent und des Ziels\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size -1, shape=(2,), dtype=int ),\n",
    "                \"target\": spaces.Box(0, size -1, shape=(2,), dtype=int ),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Weil wir vier Bewegungsmöglichkeiten haben ist die action_space = 4\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        #der Agent kann vier actionen durchführen \"right\", \"up\", \"left\", \"down\"\n",
    "        #hier werden actionen in richtungen umgewandelt\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1,0]), # nach rechts bewegen\n",
    "            1: np.array([0,1]), # nach oben bewegen\n",
    "            2: np.array([-1,0]), #nach links bewegen\n",
    "            3: np.array([0,-1]), #nach unten bewegen\n",
    "        }\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "        \"\"\"Hier änderung notwendig mehrfache Ziele\"\"\"\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "    # wenn wir auf informationen die nur in der step methode sind zugreifen wollen müssen wir das dictionary das bei _get_info in step zurückgegeben ist updaten\n",
    "    \n",
    "    \n",
    "    # reset initialieiert eine neue episode und wir immer aufgerufen wenn step done zurückgibt\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # hier geben wir self.np_random einen seed \n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # eine zufällige position für den agent auswählen\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size = 2, dtype = int)\n",
    "        \n",
    "        # solang das target random generieren bis es nicht auf der selben position wie der agent ist\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(0, self.size, size = 2, dtype = int)\n",
    "            \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "            \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Mapping der action auf die richtig in die gelaufen werden soll\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        # hier wird np.clip genutzt damit der agent nicht das grid verlassen kann\n",
    "        self._agent_location = np.clip(self._agent_location + direction, 0, self.size -1)\n",
    "        \n",
    "        \"\"\"Hier änderung notwendig mehrfache Ziele\"\"\"\n",
    "        # eine Episode ist fertig wenn der argent das target erreicht hat\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else 0 # Binary sparse rewards\n",
    "        \n",
    "        #reward für unseren algorithmus 1 für einen ereichten punkt -0.1 für jede action bis zum punkt\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "            \n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "        \n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init() # initilation PyGame\n",
    "            pygame.display.init() # initilation display\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "            \n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock() # wird für die FPS benötigt \n",
    "            \n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255,255,255))\n",
    "        \n",
    "        pix_square_size = (self.window_size / self.size) # für die größe eines quadretes im raster\n",
    "        \n",
    "        # das Ziel auf den PyGame Window ausgeben\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # den Agent auf dem PyGame Window ausgeben\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0,0,255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size/3,\n",
    "        )\n",
    "        \n",
    "        # die Rahmenlienen auf dem PyGame Window ausgeben\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width = 2,\n",
    "            )\n",
    "            \n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width = 2,\n",
    "            )\n",
    "            \n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect()) # kopiert die zufor diefienierten ausgaben in das sichbare Window\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            \n",
    "            self.clock.tick(self.metadata[\"render_fps\"]) # hält die FPS stabiel und setzt die FPS auf den wert in den Metadaten\n",
    "        else: # the case of rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1,0,2))\n",
    "        \n",
    "    def close(self): # nachdem man diese methode aufgerufen hat sollte man nicht mehr mit dem enviorment interagieren\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1006b",
   "metadata": {},
   "source": [
    "folgender Befehl muss noch ausgeführ werden\n",
    "pip install -e C:\\Users\\fabia\\OneDrive\\Semester3\\ProjektSeminar\\gym-TSP --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3529b",
   "metadata": {},
   "source": [
    "#### Initzialisierung des Enviorments\n",
    "Abgeleitet von: <br>\n",
    "https://towardsdatascience.com/ultimate-guide-for-reinforced-learning-part-1-creating-a-game-956f1f2b0a91 <br>\n",
    "https://www.gymlibrary.dev/content/environment_creation <br>\n",
    "Schwirigkeit: <br>\n",
    "Zusammenführung der Touturial inhalt und deren Abstraktion auf unsere Anwendung<br>\n",
    "<br>\n",
    "Eigenanteil: <br>\n",
    "Umsetzung der Steuerung da diese aus den Jupyter Notbooks nicht funktioniert wie im Toutorial + logig das Spiel zu schließen wenn der Punkt erreicht wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4227fe4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_TSP\n",
    "import pygame\n",
    "\n",
    "env = gym.make(\"TSPEnv-v0\", render_mode=\"human\", size = 5)\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "pygame.Rect\n",
    "\n",
    "observation, info = env.reset()\n",
    "run = True\n",
    "\n",
    "\n",
    "while run:\n",
    "    get_event = pygame.event.get()\n",
    "    \n",
    "    for event in get_event:\n",
    "        \n",
    "        if event.type == pygame.QUIT:\n",
    "            env.close()\n",
    "            run = False\n",
    "        \n",
    "        # hier beginnt der Eigenanteil\n",
    "        if event.type == pygame.KEYDOWN: \n",
    "\n",
    "\n",
    "            if event.key == pygame.K_DOWN: # nach unten bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 1)\n",
    "                print(observation)\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "\n",
    "            if  event.key == pygame.K_UP: # nach oben bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 3)\n",
    "                print(observation)\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "  \n",
    "            if event.key == pygame.K_RIGHT: # nach rechts bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 0)\n",
    "                print(observation)\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "\n",
    "            if event.key == pygame.K_LEFT: # nach links bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 2)\n",
    "                print(observation)\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "                    \n",
    "        env.render()\n",
    "            \n",
    "#Tasten Beschreibung  https://www.pygame.org/docs/ref/key.html#key-constants-label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd867cf",
   "metadata": {},
   "source": [
    "# First Agent Pytorch DQN\n",
    "mit hilfe von diesem Toutorial\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc904e",
   "metadata": {},
   "source": [
    "#### Grundlegende Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde257fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21b9f1",
   "metadata": {},
   "source": [
    "#### Imports für Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e90921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31baff",
   "metadata": {},
   "source": [
    "#### Pytorch imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "452db47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c69e0",
   "metadata": {},
   "source": [
    "#### Imports für gym Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf77420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': array([1150, 1160], dtype=int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_TSP\n",
    "import pygame\n",
    "\n",
    "# set up enviorment \n",
    "env = gym.make(\"TSPEnv-v0\", render_mode=\"rgb_array\", dataSet = \"berlin52.tsp\")\n",
    "observation, info = env.reset()\n",
    "print(observation)\n",
    "\n",
    "# set up pygame\n",
    "pygame.init()\n",
    "\n",
    "# set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff74f8",
   "metadata": {},
   "source": [
    "### Replay memory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f355160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapt State und ation an next state und reward\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa6f8e",
   "metadata": {},
   "source": [
    "### DQN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f5b72",
   "metadata": {},
   "source": [
    "Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d7804",
   "metadata": {},
   "source": [
    "abgeleitet von https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda (absatz DQN -Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e850d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # drei Convolution layers\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) #input layer\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2) #hidden layer\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2) #output Layer\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645219ea",
   "metadata": {},
   "source": [
    "Input extraction wandelt die gridword so um das dqn ihn als rgb array \"sehen\" kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31d10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize = T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()])\n",
    "\n",
    "def get_agent_location():\n",
    "    return\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    \n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "\n",
    "    return screen.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262510fe",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62672dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20000\n",
    "GAMMA = 0.999\n",
    "ESP_START = 0.9\n",
    "ESP_END = 0.05\n",
    "ESP_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 800 ALf fixer Wert aufrund Scree breite und Höhe\n",
    "policy_net = DQN(800, 800, n_actions).to(device)\n",
    "target_net = DQN(800, 800, n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = ESP_END + (ESP_START - ESP_END) * math.exp(-1. * steps_done / ESP_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device = device, dtype = torch.long)\n",
    "    \n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    # plot: durchnitt von 100 Durchgängen\n",
    "    if (len(durations_t) >= 100):\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226eec81",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1286c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a785b",
   "metadata": {},
   "source": [
    "Main training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# änderungen/abweichungen vom toutorial die übergabe des states\n",
    "\n",
    "num_episodes = 50\n",
    "for i in range(num_episodes):\n",
    "    # das Invorment zurück in den uhrsprünglichen Zustand versetzen\n",
    "    env.reset(size = i+2)\n",
    "    \n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    \n",
    "    for t in count():\n",
    "        # eine Aktion auswählen und ausführen\n",
    "        action = select_action(state)\n",
    "        _, rewardEnv, done, _, _ = env.step(action.item())\n",
    "        reward = torch.tensor([rewardEnv], device=device)\n",
    "        \n",
    "        # Echtzeit output von dem wasa der Agent macht\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10.,24.))\n",
    "        plt.imshow(env.render())\n",
    "        plt.title(f'Step: {t}, Reward: {rewardEnv}')\n",
    "        #targets_open = env.get_targets_target_location_open()\n",
    "        #for i in range(0, len(targets_open)):\n",
    "        #    x, y = targets_open[i]\n",
    "        #    plt.text(x//3, y//3, f'({x//9},{y//9})')\n",
    "        plt.show()\n",
    "        \n",
    "        # den neuen Zustand analysieren \n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        # den Übergang in Menory speichern\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # in den Nächsten zustand wechseln    \n",
    "        state = next_state\n",
    "        \n",
    "        # Optimieren\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Das target network updaten\n",
    "    if t % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfe8ba",
   "metadata": {},
   "source": [
    "# Second Agent\n",
    "https://www.youtube.com/watch?v=6pJBPPrDO40&t=872s Stand: 03.12.2022 13:21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5615d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # ohne diese Zeile wird der volgende Fehler ausgelößt\n",
    "\"\"\"OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
    "OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. \n",
    "That is dangerous, since it can degrade performance or cause incorrect results. The best thing to \n",
    "do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding \n",
    "static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented \n",
    "workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program \n",
    "to continue to execute, but that may cause crashes or silently produce incorrect results. \n",
    "For more information, please see http://www.intel.com/software/products/support/.\"\"\"\n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73191a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenstrucktur in die das Memory gespeichert wir\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbb267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_TSP\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be92319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x24cf1c31910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "plt.ion() # macht den Plot interaktiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498435c",
   "metadata": {},
   "source": [
    "#### Das Enviorment global initialisieren\n",
    "Damit die einzelen Methoden auf das Env zugreifen könne muss die global initialisiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5166aa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"TSPEnv-v0\", render_mode=\"rgb_array\", size=10)\n",
    "observation, info = env.reset()\n",
    "\n",
    "# set up pygame\n",
    "pygame.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af545302",
   "metadata": {},
   "source": [
    "#### Globale Parameter gesetzt als konstanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f1b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LearningRate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b1a59",
   "metadata": {},
   "source": [
    "#### Liner Q Net\n",
    "Feedforward net mit 3 Layern (input layer, hidden layer, output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2071446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQNet(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(inputSize,hiddenSize) # input Layer\n",
    "        self.linear2 = nn.Linear(hiddenSize,outputSize) # hidden Layer\n",
    "        self.linear3 = nn.Linear(outputSize,outputSize) # output Layer\n",
    "    \n",
    "    def forward(self, x): # ist die \"prediction\" Funktion\n",
    "        x = F.relu(self.linear1(x)) # Aktivierungsfunktion\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def save(self, fileName='model.pth'): # dient dazu das Model zu speichern\n",
    "        modelFolderPath = './model'\n",
    "        \n",
    "        if not os.path.exsits(modelFolderPath):     # wenn es den Ordner noch nicht gibt wird hier ein neuer erzeugt\n",
    "            os.makedirs(modelFolderPath)\n",
    "            \n",
    "        fileName = os.path.join(modelFolderPath, fileName) # hier wird der gesammte Filename zusammengesetzt\n",
    "        torch.save(self.state_dict(), fileName) # das Model speicher \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2e1c2",
   "metadata": {},
   "source": [
    "#### Trainer Klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786d5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, model, learningRate, gamma):\n",
    "        self.model = model\n",
    "        self.learningRate = learningRate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = self.learningRate) # als Otimizer Adam gewählt kann aber auch asugewechselt werden\n",
    "        self.criterion = nn.MSELoss() # loss Funktion mit \"Mean Squared Error\"\n",
    "        \n",
    "    def train_step(self, state, action, reward, nextState, done):\n",
    "        state = torch.tensor(state, dtype = torch.float)\n",
    "        nextState = torch.tensor(nextState, dtype = torch.float)\n",
    "        action = torch.tensor(action, dtype = torch.long)\n",
    "        reward = torch.tensor(reward, dtype = torch.float)\n",
    "        \n",
    "        # die volgenden Schritte dienen dazu das sowohl einzelne werte als auch Batches von werten verarbeitet werden können        \n",
    "        if len(state.shape) == 1:\n",
    "            # die Werte werden in der Form (n,x) benötig weshalb hier noch eine dimension hinzugefüght werden muss\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            nextState = torch.unsqueeze(nextState, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, ) # touple mit nur einem wert\n",
    "            \n",
    "        # Schritt 1: predictet Q-Werte mit dem actuellen Wert\n",
    "        prediction = self.model(state)\n",
    "        \n",
    "        # Schritt 2: Formel reward + gamme * max(nextPredictetQValue) nur wenn done = False\n",
    "        tmp = prediction.clone() \n",
    "        \n",
    "        for index in range(len(done)):\n",
    "            QNew = reward[index]\n",
    "            if not done[index]:\n",
    "                QNex = reward[index] + self.gamma * torch.max(self.model(nextState[index])) # die Oben genannte Formel wird hier angewant\n",
    "                \n",
    "            tmp[index][torch.argmax(action).item()] = QNew\n",
    "            \n",
    "        # Schritt 3: Loss Funktion    \n",
    "        self.optimizer.zero_grad() # Funktion um die Gradient zu leeren\n",
    "        loss = self.criterion(tmp, prediction) # repräsentieren QNew and Q\n",
    "        loss.backward() # backpropagation anwenden und gradient setzen\n",
    "        \n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5b19c",
   "metadata": {},
   "source": [
    "#### Get State\n",
    "Die getState() Methode weicht vom Toutorial ab und ist und baut auf \"The agent’s current position as current_row * nrows + current_col (where both the row and col start at 0).\" auf. gymlibrary.dev/environments/toy_text/frozen_lake/ Stand 03.12.2022 13:52 Die überlegung hier war eine Matematischeformel zu findn die den Abstand zu den einzelnen Zeihen darstellt und dann über die Sates mitgegeben werden kann die Formel die ich hierfür erstellt habe ist (TargetRow-AgentRow)^2+(TargetColum-AgentColum)^2. Die Quadrierung wird benötigt damit alle Positionen rund um ein Ziel gleich gewichtet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b240231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getState():\n",
    "    targetsOpen = env.get_targets_target_location_open()\n",
    "    targetsDone = env.get_target_location_done()\n",
    "    agentPosition = env.get_agentPosition()\n",
    "    \n",
    "    agentRow, agentColum = agentPosition\n",
    "    state = []\n",
    "    \n",
    "    for i in range(0, len(targetsOpen)):\n",
    "        targetRow, targetColum = targetsOpen[i]\n",
    "        state.append(((targetRow-agentRow)**2)+((targetColum-agentColum)**2)) # die oben gennante Funktion\n",
    "        \n",
    "    # damit der State immer die Selbe größe hat werden die geschlossenen Ziele mit 0 aufgefüllt    \n",
    "    for i in range(0, len(targetsDone)):\n",
    "        state.append(0)\n",
    "        \n",
    "    \"\"\"für das Testen der Funktion während des Betriebs\"\"\"\n",
    "    print(state)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113b7c5",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c48ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # um Nachzuvolziehen wie viele durchläufe es gab\n",
    "        self.NumberOfEpisodes = 0\n",
    "        # steuert den übergang zwichen random actions und gelernten actions\n",
    "        self.epsilon = 0 \n",
    "        # discount rate muss kleiner als Eins sein uns ist meist 0.8 oder 0.9\n",
    "        self.gamma = 0.9\n",
    "        # das \"Gedächtnis\" wenn das deque \"voll\" ist wird ein popleft() ausgeführt\n",
    "        self.memory = deque(maxlen = MAX_MEMORY)\n",
    "        \n",
    "        # inputSize entspricht der größe des States welcher wiederum der anzahl der Ziele entspricht\n",
    "        inputSize = len(env.get_targets_target_location_open())\n",
    "        # hiddenSize als konstante ausgesucht dieser wert kann angepasst werden\n",
    "        hiddenSize = 256 \n",
    "        # outputSize entpricht der anzahl der Möglichen actions hier 4\n",
    "        outputSize = 4\n",
    "        \n",
    "        self.model = LinearQNet(inputSize, hiddenSize, outputSize) \n",
    "        self.trainer = QTrainer(self.model, learningRate=LearningRate, gamma=self.gamma)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((self, state, action, reward, next_state, done)) # wird als Touple angehängt\n",
    "        \n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            miniSample = random.sample(self.memory, BATCH_SIZE)\n",
    "        else:\n",
    "            miniSample = self.memory \n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*miniSample) # zip sorgt dafür das die Daten richtig extrahiert werden damit alle rewards zusammen sind\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)           \n",
    "        \n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        # das epsison steuert das verhältniss zwichen exploration / explotatio\n",
    "        # exploration wird durch random moves gesteuert damit der Agent das Enviornment erkundet\n",
    "        # explotation ist das anwenden des gesammelten Wissen\n",
    "        \n",
    "        self.epsilon = 80 - self.NumberOfEpisodes\n",
    "        \n",
    "        # die Art wie eine action ausgewählt wird weicht hier vom Tutorial ab da dort eine andere steuerungslogik verwendet wird\n",
    "        \n",
    "        # ließt die möglichen actions aus dem Enviorment aus\n",
    "        n_actions = env.action_space.n\n",
    "        \n",
    "        if random.randint(0,200) < self.epsilon: # daher das das epsilon negativ werden kann, wenden hier irgendwann keine random actions mehr gewählt\n",
    "            action = random.randrange(n_actions) # auswahl einer Zufälligen action\n",
    "        else: \n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            action = torch.argmax(prediction).item() # hier müssen vielleicht noch änderungen gemacht werden \n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401838d7",
   "metadata": {},
   "source": [
    "#### Helper Klasse für Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd797e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(FinalRewards, MeanRewards):\n",
    "    display.clear_output(wait = True)\n",
    "    display.display(plt.gcf()) # um in die Aktuelle Figure zu Plotten wenn es eine gibt sonst wird eine erzeugt\n",
    "    plt.clf() # clear die Aktuelle Figur\n",
    "    plt.titel('TSP Training')\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(FinalRewards)\n",
    "    plt.plot(MeanRewards)\n",
    "    plt.ylim(ymin = 0) # setzt das Limit für die Y-Werte\n",
    "    plt.text(len(FinalRewards)-1, FinalRewards[-1], str(FinalRewards[-1]))\n",
    "    plt.text(len(MeanRewards)-1, MeanRewards[-1], str(MeanRewards[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9c2fb",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2880e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # um die ergebnisse der einzelnen Durchgänge zu speichen und am ende auszugeben\n",
    "    plot_FinalRewards = []\n",
    "    plot_MeanRewards = []\n",
    "    plot_maxReward = 0\n",
    "    maxReward = 0\n",
    "\n",
    "    agent = Agent()\n",
    "    \n",
    "    while True:\n",
    "        print(\"Old State:\")\n",
    "        #get old State\n",
    "        stateOld = getState()\n",
    "    \n",
    "        # action treffen\n",
    "        action = agent.get_action(stateOld)\n",
    "        \n",
    "        # action ausführen\n",
    "        _, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        print(\"New State:\")\n",
    "        #get new State\n",
    "        stateNew = getState()\n",
    "        \n",
    "        # das short_momory trainieren\n",
    "        agent.train_short_memory(stateOld, action, reward, stateNew, done)\n",
    "        \n",
    "        # remeremember \n",
    "        agent.remember(stateOld, action, reward, stateNew, done)\n",
    "        \n",
    "        # Ausgabe von dem was der Agent grade macht        \n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10.,24.))\n",
    "        plt.imshow(env.render())\n",
    "        plt.show()\n",
    "        \n",
    "        if done:\n",
    "            # das long_memory trainieren \n",
    "            observation, info = env.reset()\n",
    "            \n",
    "            agent.NumberOfEpisodes += 1\n",
    "            \n",
    "            # das lang Zeit memorytrainieren\n",
    "            agent.train_long_memory()\n",
    "            \n",
    "            # maximalen reward setzen\n",
    "            if reward > maxReward:\n",
    "                plot_maxReward = reward\n",
    "                agent.model.save()\n",
    "                \n",
    "            print('Episode: ', agent.NumberOfEpisode, 'Reward: ', reward, 'Derzeitiger MaxReward: ', plot_maxReward)\n",
    "            \n",
    "            plot_FinalRewards.append(reward)\n",
    "            totalReward += reward\n",
    "            plot_MeanRewards.append(totalReward / agent.NumberOfEpisodes)\n",
    "            plot(plot_FinalRewards, plot_MeanRewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f10690",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [12], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10.\u001b[39m,\u001b[38;5;241m24.\u001b[39m))\n\u001b[0;32m     34\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[1;32m---> 35\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# das long_memory trainieren \u001b[39;00m\n\u001b[0;32m     39\u001b[0m     observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:389\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    175\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    340\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py:2319\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2317\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2318\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2319\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2320\u001b[0m             filename,\n\u001b[0;32m   2321\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2322\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2323\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2324\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2325\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2326\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py:1648\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[0;32m   1641\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.3\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.6\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1642\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m() got unexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1643\u001b[0m                 \u001b[38;5;241m+\u001b[39m arg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is no longer supported as of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1644\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will become an error \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1645\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1646\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(arg)\n\u001b[1;32m-> 1648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:415\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m     deprecation_addendum \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf any parameter follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, they should be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword, not positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    408\u001b[0m     warn_deprecated(\n\u001b[0;32m    409\u001b[0m         since,\n\u001b[0;32m    410\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(name),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m                  \u001b[38;5;28;01melse\u001b[39;00m deprecation_addendum,\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:541\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03mWrite the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;124;03m    *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    540\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 541\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\image.py:1675\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1673\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1674\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1675\u001b[0m image\u001b[38;5;241m.\u001b[39msave(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2320\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2317\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2320\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\PngImagePlugin.py:1374\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1372\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode)\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFile.py:518\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 518\u001b[0m         l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(d)\n\u001b[0;32m    520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c347c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01a87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015c9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
