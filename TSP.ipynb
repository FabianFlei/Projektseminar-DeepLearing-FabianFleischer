{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1f3798",
   "metadata": {},
   "source": [
    "# Travelng Salesman Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4dcbd67",
   "metadata": {},
   "source": [
    "### Die zugrundeliegenden Problemstellung \n",
    "Das Traveling Salesman Problem befasst sich damit die Optimale Route auf einem vollständigen Graphen zu bestimmen.\n",
    "\n",
    "<div><img src=\"traditionellerAnsatz.png\" width=\"300\")</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6484c05",
   "metadata": {},
   "source": [
    "[Bild] Lotz, S. (no date) Shortest round trips, The Traveling Salesman Problem. Available at: https://algorithms.discrete.ma.tum.de/graph-games/tsp-game/index_en.html (Accessed: October 20, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77feda05",
   "metadata": {},
   "source": [
    "## Der Ansatz in diesem Projekt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90c17154",
   "metadata": {},
   "source": [
    "Das Travelng Salesman Problem vom einen Graphen in eine Grid-World umwandeln indem ein Agent sich bewegt und das Problem lösen soll.\n",
    "<div>\n",
    "    <img src=\"projektAnsatz.png\" width=\"300\")\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837250cc",
   "metadata": {},
   "source": [
    "## Die Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60f5e4",
   "metadata": {},
   "source": [
    "Ich wollte im kontrast zum Graphentheoretischen Ansatz einen Anatz kreiren der ohne neubrechnungen damit umgehen kann das Ziele auf der Route währen der Berrechnung hinzugefüht oder herausgenommen werden können ohne das, dass gesammte problem neu berrechnet werden muss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e10460",
   "metadata": {},
   "source": [
    "## Die Daten\n",
    "Der Link zu den Daten: <br>\n",
    "http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/ 31.12.2022 16:16 <br>\n",
    "<br>\n",
    "Die Dokumentation zu den Daten:<br>\n",
    "http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp95.pdf 31.12.2022 16:16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6a4af",
   "metadata": {},
   "source": [
    "Da die Daten nicht einfach dierekt lesbar sind habe ich eine Klasse geschrieben die die daten so aufbereitet das ich diese im meinem weiteren code einsetzen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc0710",
   "metadata": {},
   "source": [
    "### Formatierung\n",
    "Eine Zeile aus einem Datensatz sieht beispielsweise so aus: <br>\n",
    "'19 510.0 875.0  \\n'<br>\n",
    "Die 19 ist die Konten nummer, 510.0 der X-Wert und 875.0 der Y Wert<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfef8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Als Grundlageninformation https://www.python-lernen.de/dateien-auslesen.htm\n",
    "class DataExtractor():\n",
    "    \n",
    "    def extraktData(dataSet = None):\n",
    "        metadata = {\"dataSet\": [\"a280.tsp\", \"brd14051.tsp\", \"berlin52.tsp\"]}\n",
    "\n",
    "        assert dataSet is None or dataSet in metadata[\"dataSet\"]\n",
    "\n",
    "\n",
    "        data = open(dataSet, 'r')\n",
    "\n",
    "        # Formatiere Name\n",
    "        name = str(data.readline())\n",
    "        find = name.find(\": \")\n",
    "        if find != -1:\n",
    "            name = name[find+2:len(name)-1]\n",
    "        else: \n",
    "            name = name[name.find(\":\")+1:len(name)-1]\n",
    "\n",
    "        # Formatiere Comment\n",
    "        comment = str(data.readline())\n",
    "        find = comment.find(\": \")\n",
    "        if find != -1:\n",
    "            comment = comment[find+2:len(comment)-1]\n",
    "        else: \n",
    "            comment = comment[comment.find(\":\")+1:len(comment)-1]\n",
    "        \n",
    "        # Formatiere Type\n",
    "        type = str(data.readline())\n",
    "        find = type.find(\": \")\n",
    "        if find != -1:\n",
    "            type = type[find+2:len(type)-1]\n",
    "        else: \n",
    "            type = type[type.find(\":\")+1:len(type)-1]\n",
    "\n",
    "        # Formatiere Dimension   \n",
    "        dimension = str(data.readline())\n",
    "        find = dimension.find(\": \")\n",
    "        if find != -1:\n",
    "            dimension = int(float(dimension[find+2:len(dimension)-1]))\n",
    "        else: \n",
    "            dimension = int(float(dimension[dimension.find(\":\")+1:len(dimension)-1]))\n",
    "\n",
    "        # Formatiere Edge_weight_type\n",
    "        edge_weight_type = str(data.readline())\n",
    "        find = edge_weight_type.find(\": \")\n",
    "        if find != -1:\n",
    "            edge_weight_type = edge_weight_type[find+2:len(edge_weight_type)-1]\n",
    "        else: \n",
    "            edge_weight_type = edge_weight_type[edge_weight_type.find(\":\")+1:len(edge_weight_type)-1]\n",
    "\n",
    "        # Formatiere Node_coord_section\n",
    "        node_coord_section = str(data.readline())\n",
    "        node_coord_section = node_coord_section[:len(node_coord_section)-1]\n",
    "\n",
    "        maxValue = 0\n",
    "\n",
    "        target_location = []\n",
    "\n",
    "        while True:\n",
    "            dataPoints = data.readline()\n",
    "            # Fall ist EOF erreicht == Alle Daten eingelesen\n",
    "            if dataPoints.find(\"EOF\") != -1:\n",
    "                break\n",
    "\n",
    "\n",
    "            index = int(str(dataPoints).index(\" \"))\n",
    "            while index == 0:\n",
    "                dataPoints = dataPoints[1:]\n",
    "                index = int(str(dataPoints).index(\" \"))\n",
    "\n",
    "            dataPoints = dataPoints[index:]\n",
    "            index = 0\n",
    "\n",
    "            while index == 0:\n",
    "                dataPoints = dataPoints[1:]\n",
    "                index = int(str(dataPoints).index(\" \"))\n",
    "\n",
    "            x = int(float(dataPoints[:int(str(dataPoints).index(\" \"))]))\n",
    "\n",
    "            if x > maxValue:\n",
    "                maxValue = x\n",
    "\n",
    "            dataPoints = dataPoints[index:]\n",
    "            find = 0\n",
    "\n",
    "            while find != -1 and find == 0:\n",
    "                dataPoints = dataPoints[1:]\n",
    "                find = int(str(dataPoints).find(\" \")) \n",
    "\n",
    "            if find != -1:\n",
    "                y = int(float(dataPoints[:len(dataPoints)-1])) # vor \\n am ende jeder Zeile ist noch ein Leerzeichen\n",
    "            else:\n",
    "                y = int(float(dataPoints[:len(dataPoints)-1])) # -2 da \\n am ende jeder Zeile\n",
    "\n",
    "            if y > maxValue:\n",
    "                maxValue = y\n",
    "\n",
    "\n",
    "            target_location.append(np.array([x,y]))\n",
    "\n",
    "        info = {\n",
    "                \"name\": name, \n",
    "                \"comment\": comment, \n",
    "                \"type\": type, \n",
    "                \"dimension\": dimension,\n",
    "                \"edge_weight_type\": edge_weight_type, \n",
    "                \"node_coord_section\": node_coord_section,\n",
    "                \"maxValue\": maxValue\n",
    "                }\n",
    "\n",
    "        return target_location, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde1bcd",
   "metadata": {},
   "source": [
    "# Enviorment\n",
    "Für Reininforced Learning benötigt man einen Agent und ein Enviorment <br><br>\n",
    "Für das Enviorment nutze ich GYM und orientiere mich dabei an: <br>\n",
    "https://www.gymlibrary.dev/content/environment_creation/ <br><br>\n",
    "Änderungen die ich vornehmen muss: <br>\n",
    "-- Mehrere Ziele für den Agenten <br>\n",
    "Schwirigkeiten: <br>\n",
    "-- Alles in einen Jupiter Nodebook möglichmachen\n",
    "-- Observation space richtig zu definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc36c1a",
   "metadata": {},
   "source": [
    "### Requierments\n",
    "Folgendes muss installiert werden: <br>\n",
    "pip install pygame <br>\n",
    "pip install gym <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb38b51",
   "metadata": {},
   "source": [
    "#### Ordnerstruktur für die Enviorment Dateien um dieses zu Regiesteren damit es im Folgenden genutzt werden kann\n",
    "gym-Tsp/ <br> \n",
    "        README.md <br>\n",
    "        setup.py <br>\n",
    "        gym_TSP/ <br>\n",
    "            __init__.py <br>\n",
    "            envs/ <br>\n",
    "               __init__.py <br>\n",
    "               TSP_Env.py <br>\n",
    "               \n",
    "Daran gehalten https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d43336",
   "metadata": {},
   "source": [
    "#### Im folgenden wird gezeit was sich in welcher Datei befindet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d848bd2",
   "metadata": {},
   "source": [
    "gym-TSP\\README.md sollte eine kurze Beschreibung des Enviomrents enthalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c3181",
   "metadata": {},
   "source": [
    "gym-TSP\\setup.py der Name (name=\"gym_TSP\") den wir hier vergeben ist der Name der später bei import gym_TSP benutzt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e09d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-TSP\\setup.py\n",
    "\n",
    "#from setuptools import setup\n",
    "\n",
    "#setup(\n",
    "#    name=\"gym_TSP\",\n",
    "#    version=\"0.0.1\",\n",
    "#    install_requires=[\"gym==0.26.0\", \"pygame==2.1.0\"],\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3deed6d",
   "metadata": {},
   "source": [
    "gym-TSP\\gym_TSP\\__init__.py hier wird das Enviorment in gym registiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c75feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-TSP\\gym_TSP\\__init__.py\n",
    "\n",
    "#from gym.envs.registration import register\n",
    "\n",
    "#register(\n",
    "#    id='TSPEnv-v0', #wir in gym.make() für die Instatzierung aufgerufen \n",
    "#    entry_point='gym_TSP.envs:TSPEnv',\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c198d",
   "metadata": {},
   "source": [
    "gym-TSP\\gym_TSP\\envs\\__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb423f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-TSP\\gym_TSP\\envs\\__init__.py\n",
    "\n",
    "#from gym_TSP.envs.TSP_Env import TSPEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adf1c0",
   "metadata": {},
   "source": [
    "gym-TSP\\gym_TSP\\envs\\TSP_Env.py enthält das custom Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mehere Ziele abgeleitet von https://github.com/TheoLvs/reinforcement-learning/blob/master/5.%20Delivery%20Optimization/delivery.py\n",
    "\n",
    "#gym-TSP\\gym_TSP\\envs\\TSP_Env.py\n",
    "\n",
    "import gym # für das Enviorment zuständig\n",
    "from gym import spaces\n",
    "\n",
    "import pygame # für die Visualisierung zuständig und auführung der Actionen\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy.linalg import norm\n",
    "from DataExtractor import dataExtractor\n",
    "\n",
    "# Datenstrucktur in die das Memory gespeichert wir\n",
    "from collections import deque\n",
    "\n",
    "# wird zur berechnung der euklidichen distanz benötigt\n",
    "import mpmath \n",
    "\n",
    "class TSPEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 24,}\n",
    "    \n",
    "    def __init__(self, render_mode = None, dataSet = None, size = None, maxMemory = 100):\n",
    "        # initalisierung Ausgabe Variablen\n",
    "        self.steps = 0 # zur Ausgabe: wieviele Steps wurden gemacht\n",
    "        self.openTargets = 0 # zur Ausgabe: wieviele Targets sind noch offen\n",
    "        self.closedTargets = 0 # zur Ausgabe: wieviele Targets sind schon abgearbeitet\n",
    "\n",
    "        # initalisierung Env Variablen\n",
    "        self.size = size\n",
    "        self.dataSet = dataSet\n",
    "        self.window_size = 800 #size of the PyGame window\n",
    "        self.dataSetOrSize = True if self.dataSet != None else False\n",
    "\n",
    "        # initalisierung Mermory für gemachte schritte\n",
    "        self.memory = self.memory = deque(maxlen = maxMemory)\n",
    "\n",
    "        # spezielle initalisierungen wenn ein Datensatz verwendet wird\n",
    "        if self.dataSetOrSize:\n",
    "            _, info = dataExtractor.extractData(dataSet)\n",
    "\n",
    "            self.size = info[\"maxValue\"]//10 #size of the square grid\n",
    "            self.maxValue = info[\"maxValue\"]//10\n",
    "            self.minValue = info[\"minValue\"]//10\n",
    "            self.offset = info[\"offset\"]\n",
    "            self.dimension = info[\"dimension\"]  # -1 wenn der 52. Punkt nicht richtig angezeigt wird\n",
    "            self.quotient = 1\n",
    "            self.oneOrZero = 1\n",
    "\n",
    "            #Observations sind dictionaries mit der Positions des agent und des Ziels\n",
    "            # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "            self.observation_space = spaces.Dict(\n",
    "                {\n",
    "                    \"agent\": spaces.Box(0, self.size-1, shape=(2,), dtype=int),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            self.openTargets = self.dimension # setzt initial die Anzahl der noch offenen Ziele\n",
    "\n",
    "        else:\n",
    "            self.quotient = 1\n",
    "            self.oneOrZero = 0\n",
    "\n",
    "            self.observation_space = spaces.Dict(\n",
    "                {\n",
    "                    \"agent\": spaces.Box(0, self.size, shape=(2,), dtype=int ),\n",
    "                }\n",
    "            )   \n",
    "\n",
    "            self.openTargets = self.size # setzt initial die Anzahl der noch offenen Ziele\n",
    "\n",
    "\n",
    "\n",
    "        #o = self.observation_space[\"targets_open\"].sample()\n",
    "\n",
    "        # Weil wir vier Bewegungsmöglichkeiten haben ist die action_space = 4\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        #der Agent kann vier actionen durchführen \"right\", \"up\", \"left\", \"down\"\n",
    "        #hier werden actionen in richtungen umgewandelt\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1,0]), # nach rechts bewegen\n",
    "            1: np.array([0,1]), # nach oben bewegen\n",
    "            2: np.array([-1,0]), #nach links bewegen\n",
    "            3: np.array([0,-1]), #nach unten bewegen\n",
    "        }\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.size\n",
    "\n",
    "    def get_Steps(self):\n",
    "        return self.steps\n",
    "\n",
    "    def get_openTargets(self):\n",
    "        return self.openTargets\n",
    "\n",
    "    def get_closedTargets(self):\n",
    "        return self.closedTargets\n",
    "\n",
    "    def get_Terminated(self):\n",
    "        return self.terminated\n",
    "\n",
    "    def get_agentPosition(self):\n",
    "        return self._agent_location\n",
    "\n",
    "    def get_targets_target_location_open(self):\n",
    "        return self._target_location_open\n",
    "\n",
    "    def get_target_location_done(self):\n",
    "        return self._target_location_done\n",
    "\n",
    "    def get_printInfo(self):\n",
    "        return self.printInfo\n",
    "\n",
    "    def get_Reward(self):\n",
    "        return self.reward\n",
    "    \n",
    "    def showRoute(self):\n",
    "        # der Startpunkt wird an das ende angehangen damit ein Kreis entsteht\n",
    "        return self._target_location_done.append(self.startPoint) \n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        # hier könnte man self._target_location_done zurükgeben\n",
    "        #funktioniert aber so nicht \n",
    "        #return {np.array(self._target_location_done)}\n",
    "        return {\"reward\": self.reward}\n",
    "    # wenn wir auf informationen die nur in der step methode sind zugreifen wollen müssen wir das dictionary das bei _get_info in step zurückgegeben ist updaten\n",
    "\n",
    "    def _getState(self):\n",
    "        # diese Methode dient ermöglicht es den Zielen eine affection zu geben und auch einen Nagativen reward für das laufen in die Außenwand zu geben\n",
    "        \n",
    "        agentRow, agentColum = self._agent_location\n",
    "        state = []\n",
    "        \n",
    "        for i in range(0, len(self._target_location_open)):\n",
    "            targetRow, targetColum = self._target_location_open[i]\n",
    "            state.append(((targetRow-agentRow)**2)+((targetColum-agentColum)**2)) # die oben gennante Funktion\n",
    "            \n",
    "        # damit der State immer die Selbe größe hat werden die geschlossenen Ziele mit 0 aufgefüllt    \n",
    "        for i in range(0, len(self._target_location_done)):\n",
    "            state.append(0)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def _get_euklidische_Distanz(self):\n",
    "        distance = 0\n",
    "\n",
    "        for i in range(0, len(self._target_location_done)):\n",
    "            xi,yi = self._target_location_done[i]\n",
    "\n",
    "            if i+1 < len(self._target_location_done):\n",
    "                xj,yj = self._target_location_done[i+1]\n",
    "            else:\n",
    "                xj,yj = self._target_location_done[-1] # für den Letzen Punkt wird der weg zum Startpunkt verbunden damit ein rundweg entsteht\n",
    "\n",
    "            x = xi - xj\n",
    "            y = yi - yj\n",
    "\n",
    "            distance += mpmath.nint(mpmath.sqrt(x * x + y * y))\n",
    "\n",
    "        return distance\n",
    "\n",
    "    # reset initialieiert eine neue episode und wir immer aufgerufen wenn step done zurückgibt\n",
    "    def reset(self, seed=None, options=None, size=None, **hyperparam):\n",
    "\n",
    "        # Ausgabewerte zurücksetzen\n",
    "        # zur Ausgabe: wieviele Steps wurden gemacht\n",
    "        self.steps = 0 \n",
    "\n",
    "        # zur Ausgabe: wieviele Targets sind schon abgearbeitet\n",
    "        self.closedTargets = 0 \n",
    "\n",
    "        # zur Ausgabe: wieviel punkte mehrfach besucht worden\n",
    "        self.visitedTwice = 0\n",
    "\n",
    "        # hier geben wir self.np_random einen seed \n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # für die variable erhöhung der size während des Trainings (nur mit convolutional neural network)\n",
    "        if size != None:\n",
    "            self.size = size\n",
    "\n",
    "        # liste für die Fertigen Ziele \n",
    "        self._target_location_done = []\n",
    "\n",
    "        if self.dataSetOrSize:\n",
    "            # Ziele ertselle unter der Berücksichtigung des Ofsets\n",
    "            self._target_location_open, info = dataExtractor.extractData(self.dataSet)\n",
    "            self._target_location_open = self._target_location_open\n",
    "            \n",
    "            \"\"\"Der letzte Punkt wird momentan gelöscht da dieser ausseralb des Sichbaren und begehbaren raums liegt\"\"\"\n",
    "            del self._target_location_open[-1]\n",
    "        else:\n",
    "            self._target_location_open = []\n",
    "\n",
    "            \n",
    "            while len(self._target_location_open) < self.size:\n",
    "                x, y = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "                \n",
    "                NoDuplicate = True\n",
    "\n",
    "                # hier wird überprüft das nicht zufällig zwei Ziele an der selben Koordinate erstellt werden\n",
    "                for i in range(len(self._target_location_open)):\n",
    "                    if np.array_equal(self._target_location_open[i], np.array([x,y])):\n",
    "                        NoDuplicate = False \n",
    "                        break                    \n",
    "            \n",
    "                if NoDuplicate:\n",
    "                    self._target_location_open.append(np.array([x,y]))\n",
    "\n",
    "        \n",
    "        # eine zufällige position für den Agent auswählen \n",
    "        self.indexStartPoint = random.randint(0, len(self._target_location_open)-self.oneOrZero)\n",
    "        self._agent_location = self._target_location_open[self.indexStartPoint]//self.quotient # für random Agenent start location self.np_random.integers(0, self.size, size = 2, dtype = int))\n",
    "        \n",
    "        #start punkt setzen\n",
    "        self.startPoint = self._agent_location\n",
    "\n",
    "        # reward initialisieren \n",
    "        self.reward = 0\n",
    "\n",
    "        # das Eepsilon bestimmt für wie viele Schritte die Targets eine Affection haben\n",
    "        self.epsilon = 6000\n",
    "\n",
    "        # zur Ausgabe: wieviele Targets sind noch offen\n",
    "        self.openTargets = self.dimension if self.dataSetOrSize else len(self._target_location_open) \n",
    "            \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        oldState = self._getState()\n",
    "\n",
    "        # initialisierung des stepRewards\n",
    "        stepReward = 0 \n",
    "\n",
    "        # der indexTargetAffection bestimmt von welchem Target die Affection ausgeht\n",
    "        tmpListe = oldState.copy()\n",
    "\n",
    "        # alle Elemente aus der Liste die 0 sind müssen entfernt werden da diese Punkte\n",
    "        # bereits abgearbeitet sind und keine affection mehr haben sollen       \n",
    "        tmp = min(tmpListe)\n",
    "        \n",
    "        while tmp == 0 and len(tmpListe) != 1: \n",
    "            tmpListe.remove(0)          \n",
    "            tmp = min(tmpListe)\n",
    "\n",
    "        indexTargetAffection = oldState.index(tmp)\n",
    "\n",
    "        # überprüfung das der Startpunkt keine Affection bekommt\n",
    "        if indexTargetAffection == self.indexStartPoint and (len(tmpListe) != 1): \n",
    "            tmpListe.pop(self.indexStartPoint)\n",
    "            tmp = min(tmpListe)\n",
    "            indexTargetAffection = oldState.index(tmp)\n",
    "\n",
    "        tmpOldState = tmp\n",
    "        tmpListeOldState = tmpListe\n",
    "        indexTargetAffectionOldState = indexTargetAffection\n",
    "\n",
    "        # für die Ausgabe der gemachten Schritte\n",
    "        self.steps += 1\n",
    "\n",
    "        # Mapping der action auf die richtig in die gelaufen werden soll\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        # hier wird np.clip genutzt damit der agent nicht das grid verlassen kann\n",
    "        self._agent_location = np.clip(self._agent_location + direction, 0, self.size -1)\n",
    "\n",
    "        # reward für den algorithmus 1 für einen ereichten punkt -0.1 für jede action bis zum punkt und -1 wenn ein bereits abgearbeiteter punkt erneut besucht wird\n",
    "        \n",
    "        # negativer reward für jeden gemachten step\n",
    "        stepReward -= 10\n",
    "\n",
    "        afterTargetDoneIndexTargetAffection = indexTargetAffection        \n",
    "\n",
    "        # Ziele von der Open Liste nehmen und als done makieren + prositiver reward für abgearbeiteten Punkt\n",
    "        for i in range(0,len(self._target_location_open)):\n",
    "            if np.array_equal(np.array(self._agent_location), np.array(self._target_location_open[i]//self.quotient)):\n",
    "                if np.array_equal(np.array(self._agent_location), np.array(self.startPoint)):\n",
    "                    # dieser Teil ist dafür da das der Letzte Punkt wieder der Startpunkt sein muss\n",
    "                    if len(self._target_location_open)-self.oneOrZero == 1:\n",
    "                        self._target_location_done.append(self._target_location_open[i])\n",
    "                        stepReward += 100000 # reward für das Beenden der Route\n",
    "                        del self._target_location_open[i]\n",
    "\n",
    "                        # für die Ausgabe Offene/Fertige Ziele\n",
    "                        self.openTargets -= 1\n",
    "                        self.closedTargets += 1\n",
    "\n",
    "                        break\n",
    "                    else:\n",
    "                        break \n",
    "                \n",
    "                self._target_location_done.append(self._target_location_open[i])\n",
    "                stepReward += 100000\n",
    "                del self._target_location_open[i]\n",
    "\n",
    "                # wenn ein Target mit eienem kleinerem Index aus der Liste entfernt wird muss der self.indexStartPoint \n",
    "                # dem enstrechend um einen nach Links verschoben werden damit die berechnung der affection weiterhin\n",
    "                # wie erwartet funktioniert\n",
    "                if i < self.indexStartPoint:\n",
    "                    self.indexStartPoint -= 1\n",
    "\n",
    "                # wenn ein Target mit eienem kleinerem Index aus der Liste entfernt wird muss afterTargetDoneIndexTargetAffection\n",
    "                # dem enstrechend um einen nach Links verschoben werden damit die berechnung der affection weiterhin\n",
    "                # wie erwartet funktioniert\n",
    "                if i < indexTargetAffection:\n",
    "                    afterTargetDoneIndexTargetAffection -= 1\n",
    "\n",
    "                # für die Ausgabe Offene/Fertige Ziele\n",
    "                self.openTargets -= 1\n",
    "                self.closedTargets += 1\n",
    "                break\n",
    "                \n",
    "        # eine Episode ist fertig wenn der argent alle targets erreicht hat\n",
    "        self.terminated = True if len(self._target_location_open)-self.oneOrZero == 0 else False\n",
    "        \n",
    "        newState = self._getState()\n",
    "\n",
    "        runAgainstWall = False\n",
    "        # negativer reward für das Laufen gegen die grenze der Welt\n",
    "        if newState == oldState:\n",
    "            stepReward -= 1000000\n",
    "            runAgainstWall = True\n",
    "\n",
    "        if indexTargetAffection < 0 or indexTargetAffection >= len(oldState) or indexTargetAffection >= len(newState):\n",
    "            print(indexTargetAffection)\n",
    "            print(len(newState))\n",
    "            print(len(oldState))\n",
    "            print(self.indexStartPoint)\n",
    "\n",
    "        # reward wenn der agent in die richtung der Ziele Läuft\n",
    "        affectionReward = oldState[indexTargetAffection] - newState[afterTargetDoneIndexTargetAffection] # dieser reward kann auch negativ werden\n",
    "        \n",
    "        # der affectionReward wird mit den gemachten Schritten immer weiter abflachen\n",
    "        affectionReward = affectionReward * (self.epsilon/2) if affectionReward < 1 else affectionReward * (self.epsilon/2)\n",
    "        \n",
    "        stepReward += affectionReward\n",
    "\n",
    "        # wenn der Agent an einem ort schon war soll er einen negativen reward erhalten\n",
    "        for element in self.memory:\n",
    "            if np.array_equal(self._agent_location, element):\n",
    "                stepReward -= affectionReward\n",
    "                stepReward -= 100000\n",
    "\n",
    "                self.visitedTwice += 1\n",
    "\n",
    "        # besuchten punkt in das memory setzten\n",
    "        self.memory.append(self._agent_location)\n",
    "\n",
    "\n",
    "        self.reward += stepReward\n",
    "\n",
    "\n",
    "        self.printInfo = {\n",
    "                        \"runAgainstWall\":  runAgainstWall,\n",
    "                        \"oldState\": oldState,\n",
    "                        \"newState\": newState,\n",
    "                        \"indexTargetAffectionOldState\": indexTargetAffectionOldState,\n",
    "                        \"affectionReward\": affectionReward,\n",
    "                        \"indexStartPoint\": self.indexStartPoint,\n",
    "                        \"tmpOldState\": tmpOldState,\n",
    "                        \"tmpListeOldState\": tmpListeOldState,\n",
    "                        \"stepReward\": stepReward,\n",
    "                        \"euklidische_Distanz\": self._get_euklidische_Distanz(), \n",
    "                        \"visitedTwice\": self.visitedTwice,\n",
    "                    }\n",
    "\n",
    "\n",
    "        # das epsilon decrementieren damit die affection der Punkte nachlässt\n",
    "        self.epsilon -= 1\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "            \n",
    "        return observation, stepReward, self.terminated, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "        \n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init() # initilation PyGame\n",
    "            pygame.display.init() # initilation display\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "            \n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock() # wird für die FPS benötigt \n",
    "            \n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255,255,255))\n",
    "        \n",
    "        pix_square_size = (self.window_size / self.size) # für die größe eines quadretes im raster\n",
    "        \n",
    "\n",
    "        coordinatesTargetsOpen = []\n",
    "        # die noch offenen Ziele auf den PyGame Window ausgeben\n",
    "        for i in range(0, len(self._target_location_open)):\n",
    "            a =np.array(self._target_location_open[i]//self.quotient)\n",
    "            pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 145, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * a,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "            )\n",
    "            coordinatesTargetsOpen.append(a)\n",
    "\n",
    "\n",
    "        # testet ob zwei oder mehrere Punkte die selben Koordinaten haben\n",
    "        # dies könnte durch die Umrechnung der Daten auftreten bei zwei punkten die sehr nah bei einander sind\n",
    "        Mehfrach = False\n",
    "        offsetA = 1\n",
    "        offsetB = 0\n",
    "        for a in range(0,len(coordinatesTargetsOpen)):\n",
    "            for b in range(offsetA,len(coordinatesTargetsOpen)):\n",
    "                if np.array_equal(coordinatesTargetsOpen[a], coordinatesTargetsOpen[b]): Mehfrach = True \n",
    "            for k in range(0, offsetB):\n",
    "                if np.array_equal(coordinatesTargetsOpen[a], coordinatesTargetsOpen[k]): Mehfrach = True\n",
    "            offsetA += 1\n",
    "            offsetB += 1\n",
    "        self._target_dupplicate = Mehfrach # eingebaut für Fehleranalyse\n",
    "\n",
    "  \n",
    "        # den Startpunkt farblich hervorheben\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 192, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self.startPoint,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "            )\n",
    "\n",
    "\n",
    "        # die fertigen Ziele auf den PyGame Window ausgeben\n",
    "        for i in range(0, len(self._target_location_done)):\n",
    "            a =np.array(self._target_location_done[i]//self.quotient)\n",
    "            pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * a,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "            )\n",
    "               \n",
    "        # den Agent auf dem PyGame Window ausgeben\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0,0,255),\n",
    "            (self._agent_location + 0.5)* pix_square_size,\n",
    "            pix_square_size/2,\n",
    "        )\n",
    "\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect()) # kopiert die zufor diefienierten ausgaben in das sichbare Window\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            \n",
    "            self.clock.tick(self.metadata[\"render_fps\"]) # hält die FPS stabiel und setzt die FPS auf den wert in den Metadaten\n",
    "        else: # the case of rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1,0,2))\n",
    "        \n",
    "    def close(self): # nachdem man diese methode aufgerufen hat sollte man nicht mehr mit dem enviorment interagieren\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1006b",
   "metadata": {},
   "source": [
    "folgender Befehl muss noch ausgeführ werden\n",
    "pip install -e C:\\Users\\fabia\\OneDrive\\Semester3\\ProjektSeminar\\gym-TSP --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3529b",
   "metadata": {},
   "source": [
    "#### Initzialisierung des Enviorments\n",
    "Abgeleitet von: <br>\n",
    "https://towardsdatascience.com/ultimate-guide-for-reinforced-learning-part-1-creating-a-game-956f1f2b0a91 <br>\n",
    "https://www.gymlibrary.dev/content/environment_creation <br>\n",
    "Schwirigkeit: <br>\n",
    "Zusammenführung der Touturial inhalt und deren Abstraktion auf unsere Anwendung<br>\n",
    "<br>\n",
    "Eigenanteil: <br>\n",
    "Umsetzung der Steuerung da diese aus den Jupyter Notbooks nicht funktioniert wie im Toutorial + logig das Spiel zu schließen wenn der Punkt erreicht wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4227fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_TSP\n",
    "import pygame\n",
    "\n",
    "env = gym.make(\"TSPEnv-v0\", render_mode=\"human\", size = 7)\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "\n",
    "\n",
    "observation, info = env.reset()\n",
    "run = True\n",
    "\n",
    "\n",
    "while run:\n",
    "    get_event = pygame.event.get()\n",
    "    \n",
    "    for event in get_event:\n",
    "        \n",
    "        if event.type == pygame.QUIT:\n",
    "            env.close()\n",
    "            run = False\n",
    "        \n",
    "        # hier beginnt der Eigenanteil\n",
    "        if event.type == pygame.KEYDOWN: \n",
    "\n",
    "\n",
    "            if event.key == pygame.K_DOWN: # nach unten bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 1)\n",
    "                print(env.showRoute())\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "\n",
    "            if  event.key == pygame.K_UP: # nach oben bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 3)\n",
    "                print(env.showRoute())\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "  \n",
    "            if event.key == pygame.K_RIGHT: # nach rechts bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 0)\n",
    "                print(env.showRoute())\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "\n",
    "            if event.key == pygame.K_LEFT: # nach links bewegen\n",
    "                observation, reward, terminated, boo, info = env.step(action = 2)\n",
    "                print(env.showRoute())\n",
    "                if terminated:\n",
    "                    env.close()\n",
    "                    run = False\n",
    "                    \n",
    "        env.render()\n",
    "            \n",
    "#Tasten Beschreibung  https://www.pygame.org/docs/ref/key.html#key-constants-label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd867cf",
   "metadata": {},
   "source": [
    "# Erster Ansatz convolutional network\n",
    "mit hilfe von diesem Toutorial\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html 06.01.2023 20:30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc904e",
   "metadata": {},
   "source": [
    "#### Grundlegende Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde257fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21b9f1",
   "metadata": {},
   "source": [
    "#### Imports für Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31baff",
   "metadata": {},
   "source": [
    "#### Pytorch imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452db47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c69e0",
   "metadata": {},
   "source": [
    "#### Imports für gym Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf77420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_TSP\n",
    "import pygame\n",
    "\n",
    "# set up enviorment \n",
    "env = gym.make(\"TSPEnv-v0\", render_mode=\"rgb_array\", size = 7)\n",
    "observation, info = env.reset()\n",
    "print(observation)\n",
    "\n",
    "# set up pygame\n",
    "pygame.init()\n",
    "\n",
    "# set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff74f8",
   "metadata": {},
   "source": [
    "### Replay memory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f355160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapt State und ation an next state und reward\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa6f8e",
   "metadata": {},
   "source": [
    "### DQN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f5b72",
   "metadata": {},
   "source": [
    "Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d7804",
   "metadata": {},
   "source": [
    "abgeleitet von https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda (absatz DQN -Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # drei Convolution layers\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2) #input layer\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2) #hidden layer\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2) #output Layer\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645219ea",
   "metadata": {},
   "source": [
    "Input extraction wandelt die gridword so um das dqn ihn als rgb array \"sehen\" kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize = T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()])\n",
    "\n",
    "def get_agent_location():\n",
    "    return\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    \n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "\n",
    "    return screen.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262510fe",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62672dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20000\n",
    "GAMMA = 0.999\n",
    "ESP_START = 0.9\n",
    "ESP_END = 0.05\n",
    "ESP_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 800 fixer Wert aufgrund der window Breite und Höhe\n",
    "policy_net = DQN(800, 800, n_actions).to(device)\n",
    "target_net = DQN(800, 800, n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = ESP_END + (ESP_START - ESP_END) * math.exp(-1. * steps_done / ESP_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device = device, dtype = torch.long)\n",
    "    \n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    # plot: durchnitt von 100 Durchgängen\n",
    "    if (len(durations_t) >= 100):\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226eec81",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1286c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a785b",
   "metadata": {},
   "source": [
    "Main training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# änderungen/abweichungen vom toutorial die übergabe des states\n",
    "\n",
    "num_episodes = 50\n",
    "for i in range(num_episodes):\n",
    "    # das Invorment zurück in den uhrsprünglichen Zustand versetzen\n",
    "    env.reset()\n",
    "    \n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    \n",
    "    for t in count():\n",
    "        # eine Aktion auswählen und ausführen\n",
    "        action = select_action(state)\n",
    "        _, rewardEnv, done, _, _ = env.step(action.item())\n",
    "        reward = torch.tensor([rewardEnv], device=device)\n",
    "        \n",
    "        # Echtzeit output von dem wasa der Agent macht\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(5.,12.))\n",
    "        plt.imshow(env.render())\n",
    "        plt.title(f'Step: {t}, Reward: {rewardEnv}')\n",
    "        #targets_open = env.get_targets_target_location_open()\n",
    "        #for i in range(0, len(targets_open)):\n",
    "        #    x, y = targets_open[i]\n",
    "        #    plt.text(x//3, y//3, f'({x//9},{y//9})')\n",
    "        plt.show()\n",
    "        \n",
    "        # den neuen Zustand analysieren \n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        # den Übergang in Menory speichern\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # in den Nächsten zustand wechseln    \n",
    "        state = next_state\n",
    "        \n",
    "        # Optimieren\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Das target network updaten\n",
    "    if t % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfe8ba",
   "metadata": {},
   "source": [
    "# Zweiter Ansatz liniar Network\n",
    "https://www.youtube.com/watch?v=6pJBPPrDO40&t=872s Stand: 03.12.2022 13:21\n",
    "https://github.com/patrickloeber/snake-ai-pytorch Stand: 13.12.2022 11:33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb595684",
   "metadata": {},
   "source": [
    "install requrement für die berrechnung der Euklidichen Distanz <br>\n",
    "pip install mpmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5615d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # ohne diese Zeile wird der volgende Fehler ausgelößt\n",
    "\"\"\"OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
    "OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. \n",
    "That is dangerous, since it can degrade performance or cause incorrect results. The best thing to \n",
    "do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding \n",
    "static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented \n",
    "workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program \n",
    "to continue to execute, but that may cause crashes or silently produce incorrect results. \n",
    "For more information, please see http://www.intel.com/software/products/support/.\"\"\"\n",
    "\n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897305b",
   "metadata": {},
   "source": [
    "Für auswertungen und Hyperparametertunige verwende ich tensorboard welcher aus dem env die informattionen bekommt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fef9ab",
   "metadata": {},
   "source": [
    "Install requirement: <br>\n",
    "conda install pytorch torchvision -c pytorch <br>\n",
    "alternative pip install torch torchvision <br> <br>\n",
    "pip install tensorboard <br><br>\n",
    "um mit Tensorboard etwas aufzuzeichnen <br>\n",
    "tensorboard --logdir=C:\\Users\\fabia\\OneDrive\\Semester3\\ProjektSeminar\\runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('C:/Users/fabia/OneDrive/Semester3/ProjektSeminar/runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73191a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenstrucktur in die das Memory gespeichert wir\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_TSP\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be92319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23cb787",
   "metadata": {},
   "source": [
    "TQDM wird für die visualiserung des Trainingsfortschrittes genutz insbesonders wenn kein Plot ausgeführ wird <br>\n",
    "Install requirment: <br>\n",
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a19ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96345f24",
   "metadata": {},
   "source": [
    "ipywigest werden genutz zur textausgabe während des Trainings um die wichtigsten Informationen auszugeben <br>\n",
    "Angelegt an: <br>\n",
    "https://www.elab2go.de/demo-py1/jupyter-notebook-widgets.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aa1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Layout\n",
    "from ipywidgets import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295eae71",
   "metadata": {},
   "source": [
    "time wird für das automatisieren des Trainings benutzt damit ein agent nicht endlos trainiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd0a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af545302",
   "metadata": {},
   "source": [
    "#### Hyperparameter als globale Konstanten setzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = {\n",
    "                    \"Agent_MaxMemory\": 100000,\n",
    "                    \"Agent_BatchSize\": 250,\n",
    "                    \"Agent_LearningRate\": 0.00100,\n",
    "                    \"Agent_EpsilonStartValue\": 80, \n",
    "                    \"Agent_Gamma\": 0.9,\n",
    "                    \"Env_Memory\": 10000,\n",
    "                    \"Env_Epsilon\": 8000,\n",
    "                    \"Env_Size\": 7,\n",
    "                    \"Reward_SameWayTwice\": -100000,\n",
    "                    \"Reward_ClosedTarget\": 1000000,\n",
    "                    \"Reward_Terminated\": 1000000,\n",
    "                    \"Reward_RunAgainstWall\": -100000,\n",
    "                    \"Run_NumEpisoden\": 10,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498435c",
   "metadata": {},
   "source": [
    "#### Das Enviorment global initialisieren\n",
    "Damit die einzelen Methoden auf das Env zugreifen könne muss die global initialisiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TSPEnv-v0\", render_mode=\"rgb_array\", size=hyperparameter[\"Env_Size\"])\n",
    "observation, info = env.reset()\n",
    "\n",
    "# set up pygame\n",
    "pygame.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b1a59",
   "metadata": {},
   "source": [
    "#### Liner Q Net\n",
    "Feedforward net mit 3 Layern (input layer, hidden layer, output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQNet(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(inputSize,hiddenSize) # input Layer\n",
    "        self.linear2 = nn.Linear(hiddenSize,outputSize) # hidden Layer\n",
    "        self.linear3 = nn.Linear(outputSize,outputSize) # output Layer\n",
    "    \n",
    "    def forward(self, x): # ist die \"prediction\" Funktion\n",
    "        x = F.relu(self.linear1(x)) # Aktivierungsfunktion\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def save(self, fileName='model.pth'): # dient dazu das Model zu speichern\n",
    "        modelFolderPath = './model'\n",
    "        \n",
    "        if not os.path.exists(modelFolderPath):     # wenn es den Ordner noch nicht gibt wird hier ein neuer erzeugt\n",
    "            os.makedirs(modelFolderPath)\n",
    "            \n",
    "        fileName = os.path.join(modelFolderPath, fileName) # hier wird der gesammte Filename zusammengesetzt\n",
    "        torch.save(self.state_dict(), fileName) # das Model speicher \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2e1c2",
   "metadata": {},
   "source": [
    "#### Trainer Klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, model, learningRate, gamma):\n",
    "        self.model = model\n",
    "        self.learningRate = learningRate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = self.learningRate) # als Otimizer Adam gewählt kann aber auch asugewechselt werden\n",
    "        self.criterion = nn.MSELoss() # loss Funktion mit \"Mean Squared Error\"\n",
    "        \n",
    "    def train_step(self, state, action, reward, nextState, done):\n",
    "        state = torch.tensor(state, dtype = torch.float)\n",
    "        nextState = torch.tensor(nextState, dtype = torch.float)\n",
    "        action = torch.tensor(action, dtype = torch.long)\n",
    "        reward = torch.tensor(reward, dtype = torch.float)\n",
    "        \n",
    "        # die volgenden Schritte dienen dazu das sowohl einzelne werte als auch Batches von werten verarbeitet werden können        \n",
    "        if len(state.shape) == 1:\n",
    "            # die Werte werden in der Form (n,x) benötig weshalb hier noch eine dimension hinzugefüght werden muss\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            nextState = torch.unsqueeze(nextState, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, ) # touple mit nur einem wert\n",
    "            \n",
    "        # Schritt 1: predictet Q-Werte mit dem actuellen Wert\n",
    "        prediction = self.model(state)\n",
    "        \n",
    "        # Schritt 2: Formel reward + gamme * max(nextPredictetQValue) nur wenn done = False\n",
    "        tmp = prediction.clone() \n",
    "\n",
    "        for index in range(len(done)):\n",
    "            QNew = reward[index]\n",
    "            if not done[index]:\n",
    "                QNex = reward[index] + self.gamma * torch.max(self.model(nextState[index])) # die Oben genannte Formel wird hier angewant\n",
    "            tmp[index][action] = QNew\n",
    "            \n",
    "        # Schritt 3: Loss Funktion    \n",
    "        self.optimizer.zero_grad() # Funktion um die Gradient zu leeren\n",
    "        loss = self.criterion(tmp, prediction) # repräsentieren QNew and Q\n",
    "        loss.backward() # backpropagation anwenden und gradient setzen\n",
    "        \n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5b19c",
   "metadata": {},
   "source": [
    "#### Get State\n",
    "Die getState() Methode weicht vom Toutorial ab und ist und baut auf \"The agent’s current position as current_row * nrows + current_col (where both the row and col start at 0).\" auf. gymlibrary.dev/environments/toy_text/frozen_lake/ Stand 03.12.2022 13:52 Die überlegung hier war eine Matematischeformel zu findn die den Abstand zu den einzelnen Zeihen darstellt und dann über die Sates mitgegeben werden kann die Formel die ich hierfür erstellt habe ist (TargetRow-AgentRow)^2+(TargetColum-AgentColum)^2. Die Quadrierung wird benötigt damit alle Positionen rund um ein Ziel gleich gewichtet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b240231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getState():\n",
    "    targetsOpen = env.get_targets_target_location_open()\n",
    "    targetsDone = env.get_target_location_done()\n",
    "    agentPosition = env.get_agentPosition()\n",
    "    \n",
    "    agentRow, agentColum = agentPosition\n",
    "    state = []\n",
    "    \n",
    "    for i in range(0, len(targetsOpen)):\n",
    "        targetRow, targetColum = targetsOpen[i]\n",
    "        state.append(((targetRow-agentRow)**2)+((targetColum-agentColum)**2)) # die oben gennante Funktion\n",
    "        \n",
    "    # damit der State immer die Selbe größe hat werden die geschlossenen Ziele mit 0 aufgefüllt    \n",
    "    for i in range(0, len(targetsDone)):\n",
    "        state.append(0)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113b7c5",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c48ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, load = None):\n",
    "        # um Nachzuvolziehen wie viele durchläufe es gab\n",
    "        self.NumberOfFoundTargets = 0\n",
    "        # steuert den übergang zwichen random actions und gelernten actions\n",
    "        self.epsilon = 0 \n",
    "        # discount rate muss kleiner als Eins sein uns ist meist 0.8 oder 0.9\n",
    "        self.gamma = hyperparameter[\"Agent_Gamma\"]\n",
    "        # das \"Gedächtnis\" wenn das deque \"voll\" ist wird ein popleft() ausgeführt\n",
    "        self.memory = deque(maxlen = hyperparameter[\"Agent_MaxMemory\"])\n",
    "        \n",
    "        # inputSize entspricht der größe des States welcher wiederum der anzahl der Ziele entspricht\n",
    "        inputSize = hyperparameter[\"Env_Size\"]\n",
    "        # hiddenSize als konstante ausgesucht dieser wert kann angepasst werden\n",
    "        hiddenSize = 256 \n",
    "        # outputSize entpricht der anzahl der Möglichen actions hier 4\n",
    "        outputSize = 4\n",
    "        \n",
    "        self.model = LinearQNet(inputSize, hiddenSize, outputSize) \n",
    "        \n",
    "        if load != None:\n",
    "            self.model.load_state_dict(torch.load(load))\n",
    "            self.model.eval()\n",
    "            # Zum ausgeben der Model Paramter\n",
    "            # print(self.model.state_dict())\n",
    "        \n",
    "        self.trainer = QTrainer(self.model, learningRate=hyperparameter[\"Agent_LearningRate\"], gamma=self.gamma)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((self, state, action, reward, next_state, done)) # wird als Touple angehängt\n",
    "        \n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > hyperparameter[\"Agent_BatchSize\"]:\n",
    "            miniSample = random.sample(self.memory, hyperparameter[\"Agent_BatchSize\"])\n",
    "        else:\n",
    "            miniSample = self.memory \n",
    "        _, states, actions, rewards, next_states, dones = zip(*miniSample) # zip sorgt dafür das die Daten richtig extrahiert werden damit alle rewards zusammen sind\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)           \n",
    "        \n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        # das epsison steuert das verhältniss zwichen exploration / explotatio\n",
    "        # exploration wird durch random moves gesteuert damit der Agent das Enviornment erkundet\n",
    "        # explotation ist das anwenden des gesammelten Wissen\n",
    "        \n",
    "        self.epsilon = hyperparameter[\"Agent_EpsilonStartValue\"] - self.NumberOfFoundTargets\n",
    "        \n",
    "        # die Art wie eine action ausgewählt wird weicht hier vom Tutorial ab da dort eine andere steuerungslogik verwendet wird\n",
    "        \n",
    "        # ließt die möglichen actions aus dem Enviorment aus\n",
    "        n_actions = env.action_space.n\n",
    "        \n",
    "        if random.randint(0,200) < self.epsilon: # daher das das epsilon negativ werden kann, wenden hier irgendwann keine random actions mehr gewählt\n",
    "            action = random.randrange(n_actions) # auswahl einer Zufälligen action\n",
    "        else: \n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            action = torch.argmax(prediction).item() # hier müssen vielleicht noch änderungen gemacht werden \n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9c2fb",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Abweichugen vom Tutorial sind an dieser Stelle die Ergänzung von Episoden und die darasresultierenden Anpassungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2880e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(render = False, info = False, numEpisodes = 50, auto = False, timeoutTime = 60, load = None):\n",
    "    maxReward = 0\n",
    "    if load == None:\n",
    "        agent = Agent()\n",
    "    else:\n",
    "        agent = Agent(load)\n",
    "    \n",
    "    outOfTime = False\n",
    "    \n",
    "    for i in tqdm(range(numEpisodes)):\n",
    "        # das Enviornment für die Nächste Episode zurücksetzten (hier kann auch der Parameter Size mit gegeben werden)\n",
    "        env.reset(hyperparam=hyperparameter)\n",
    "        \n",
    "        # terminated ist true wenn das TSP Problem gelöst wurde\n",
    "        terminated = False\n",
    "        \n",
    "        # zu ermitteln ob ein neuer Punkt abgearbeitet wurde\n",
    "        done = False\n",
    "        doneTargets = env.get_target_location_done().copy()\n",
    "\n",
    "        timeout = time.time() + 60*timeoutTime\n",
    "        \n",
    "        while not terminated:\n",
    "            \n",
    "            # get old State\n",
    "            stateOld = getState()\n",
    "\n",
    "            # action treffen\n",
    "            action = agent.get_action(stateOld)\n",
    "\n",
    "            # action ausführen\n",
    "            _, reward, terminated, _, _ = env.step(action)\n",
    "\n",
    "            # get new State\n",
    "            stateNew = getState()\n",
    "            \n",
    "            if doneTargets != env.get_target_location_done():\n",
    "                doneTargts = env.get_target_location_done().copy()\n",
    "                done = True\n",
    "\n",
    "            # das short_momory trainieren\n",
    "            agent.train_short_memory(stateOld, action, reward, stateNew, done)\n",
    "\n",
    "            # remeremember \n",
    "            agent.remember(stateOld, action, reward, stateNew, done)\n",
    "\n",
    "            if terminated:\n",
    "                # maximalen reward setzen\n",
    "                if reward > maxReward:\n",
    "                    maxReward = reward\n",
    "                    \n",
    "                printInfo = env.get_printInfo()\n",
    "                \n",
    "                writer.add_scalar('test/steps', env.get_Steps(), i)\n",
    "                writer.add_scalar('test/reward', reward, i)\n",
    "                writer.add_scalar('test/euklidischeDistanz', printInfo[\"euklidische_Distanz\"], i)\n",
    "                \n",
    "                writer.add_hparams(\n",
    "                        {\n",
    "                         'Agent_MaxMemory': hyperparameter[\"Agent_MaxMemory\"],\n",
    "                         'Agent_BatchSize': hyperparameter[\"Agent_BatchSize\"],\n",
    "                         'Agent_LearningRate': hyperparameter[\"Agent_LearningRate\"],\n",
    "                         'Agent_EpsilonStartValue': hyperparameter[\"Agent_EpsilonStartValue\"],\n",
    "                         'Agent_Gamma': hyperparameter[\"Agent_Gamma\"],\n",
    "                         'Env_Memory': hyperparameter[\"Env_Memory\"],\n",
    "                         'Env_Epsilon': hyperparameter[\"Env_Epsilon\"],\n",
    "                         'Env_Size': hyperparameter[\"Env_Size\"],\n",
    "                         'Reward_SameWayTwice': hyperparameter[\"Reward_SameWayTwice\"],\n",
    "                         'Reward_ClosedTarget': hyperparameter[\"Reward_ClosedTarget\"],\n",
    "                         'Reward_Terminated': hyperparameter[\"Reward_Terminated\"],\n",
    "                         'Reward_RunAgainstWall': hyperparameter[\"Reward_RunAgainstWall\"],\n",
    "                         'Run_NumEpisoden': hyperparameter[\"Run_NumEpisoden\"]\n",
    "                        }, \n",
    "                        {\n",
    "                         'hparam/steps': env.get_Steps(),\n",
    "                         'hparam/reward': reward,\n",
    "                         'hparam/Episode': i,\n",
    "                         'hparam/euklidischeDistanz': printInfo[\"euklidische_Distanz\"]\n",
    "                        }\n",
    "                      )\n",
    "            \n",
    "            # dient dem optionalen rendern des Enviornments\n",
    "            if render: \n",
    "                clear_output(wait=True)\n",
    "                plt.figure(figsize=(5.,12.))\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "                \n",
    "            # dient der optimalen ausgabe von Informationen\n",
    "            if info:\n",
    "                printInfo = env.get_printInfo()\n",
    "                \n",
    "                # damit info auch mit render aktiv verfügbar ist\n",
    "                if not render:\n",
    "                    clear_output(wait=True) \n",
    "                print(\"Episode\")\n",
    "                print(i)\n",
    "                \n",
    "                print(\"Steps:\")\n",
    "                print(env.get_Steps())\n",
    "                \n",
    "                print(\"Reward:\")\n",
    "                print(env.get_Reward())\n",
    "                \n",
    "                print(\"Step Reward:\")\n",
    "                print(printInfo[\"stepReward\"])\n",
    "                \n",
    "                print(\"Offene Ziele:\")\n",
    "                print(env.get_openTargets())\n",
    "                print(env.get_targets_target_location_open())\n",
    "            \n",
    "                print(\"Fertige Ziele:\")\n",
    "                print(env.get_closedTargets())\n",
    "                print(env.get_target_location_done())\n",
    "                \n",
    "                print(\"Agent position:\")\n",
    "                print(env.get_agentPosition())\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"Old State:\")\n",
    "                print(stateOld)\n",
    "                \n",
    "                print(\"Old State Env:\")\n",
    "                print(printInfo[\"oldState\"])\n",
    "                \n",
    "                print(\"Index Target Affection Old State:\")\n",
    "                print(printInfo[\"indexTargetAffectionOldState\"])\n",
    "                \n",
    "                print(\"New State:\")\n",
    "                print(stateNew)\n",
    "                                \n",
    "                print(\"Nex State Env:\")\n",
    "                print(printInfo[\"newState\"])\n",
    "\n",
    "                print(\"Affection reward:\")\n",
    "                print(printInfo[\"affectionReward\"])\n",
    "                \n",
    "                print(\"Index StartPoint:\")\n",
    "                print(printInfo[\"indexStartPoint\"])\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"tmpOldState:\")\n",
    "                print(printInfo[\"tmpOldState\"])\n",
    "                \n",
    "                print(\"tmpListeOldState:\")\n",
    "                print(printInfo[\"tmpListeOldState\"])\n",
    "\n",
    "                if  printInfo[\"runAgainstWall\"]:\n",
    "                    print(\"runAgainstWall\")\n",
    "                else:\n",
    "                    print()\n",
    "                    \n",
    "                print(\"visitedTwice\")\n",
    "                print(printInfo[\"visitedTwice\"])\n",
    "                    \n",
    "                print(\"euklidische Distanz\")\n",
    "                print(printInfo[\"euklidische_Distanz\"])\n",
    "                \n",
    "                print(\"Bisherige Route\")\n",
    "                print(env.showRoute())\n",
    "                    \n",
    "\n",
    "            if done:\n",
    "                agent.NumberOfFoundTargets += 1\n",
    "\n",
    "                # das lang Zeit memorytrainieren\n",
    "                agent.train_long_memory()\n",
    "\n",
    "                done = False\n",
    "                \n",
    "            if timeout < time.time():\n",
    "                outOfTime = True\n",
    "                break\n",
    "    \n",
    "        if outOfTime:\n",
    "            break\n",
    "          \n",
    "        # Model wird nur gespeichert wenn es alle Testläufe ohne timout bestanden hat\n",
    "        fileName = \"model_\" + time.strftime('%H%M%S_%d%m%Y') + \".pth\"\n",
    "        agent.model.save(fileName = fileName)   \n",
    "        \n",
    "    if not auto:        \n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9809e4",
   "metadata": {},
   "source": [
    "Automatisiertes Training für Hyperparamterttuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f10690",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "auto = True\n",
    "\n",
    "# steuerungsparameter\n",
    "trainEpisodes = 1000\n",
    "# da Targest zufällig positioniert werden soll das selbe\n",
    "# hyperparameter set mehrfach getestet erden für aussagekräftige Ergebnisse \n",
    "setTests = 2\n",
    "\n",
    "\n",
    "# training um kombinations effecte zwichenden einzenlnen Hyperparamteren herrauszufinden\n",
    "kombinationTraining = 1000\n",
    "    \n",
    "intToHyperparameter={\n",
    "                        0:\"Agent_MaxMemory\",\n",
    "                        1:\"Agent_BatchSize\",\n",
    "                        2:\"Agent_LearningRate\",\n",
    "                        3:\"Agent_EpsilonStartValue\", \n",
    "                        4:\"Agent_Gamma\",\n",
    "                        5:\"Env_Memory\",\n",
    "                        6:\"Env_Epsilon\",\n",
    "                        7:\"Env_Size\",\n",
    "                        8:\"Reward_SameWayTwice\",\n",
    "                        9:\"Reward_ClosedTarget\",\n",
    "                        10:\"Reward_Terminated\",\n",
    "                        11:\"Reward_RunAgainstWall\",\n",
    "                     }\n",
    "\n",
    "Agent_MaxMemory = np.empty(0)\n",
    "Agent_BatchSize = np.empty(0)\n",
    "Agent_LearningRate = np.empty(0)\n",
    "Agent_EpsilonStartValue = np.empty(0)\n",
    "Agent_Gamma = np.empty(0)\n",
    "Env_Memory = np.empty(0)\n",
    "Env_Epsilon = np.empty(0)\n",
    "Env_Size = np.empty(0)\n",
    "Reward_SameWayTwice = np.empty(0)\n",
    "Reward_ClosedTarget = np.empty(0)\n",
    "Reward_Terminated = list(range(1000000, 1500000, 100000))\n",
    "Reward_RunAgainstWall = list(range(-2000000, -50000, 10000))\n",
    "    \n",
    "hyperparameterSet = [\n",
    "                     Agent_MaxMemory, \n",
    "                     Agent_BatchSize, \n",
    "                     Agent_LearningRate,\n",
    "                     Agent_EpsilonStartValue,\n",
    "                     Agent_Gamma,\n",
    "                     Env_Memory,\n",
    "                     Env_Epsilon,\n",
    "                     Env_Size,\n",
    "                     Reward_SameWayTwice,\n",
    "                     Reward_ClosedTarget,\n",
    "                     Reward_Terminated,\n",
    "                     Reward_RunAgainstWall\n",
    "                    ]\n",
    "\n",
    "# Kopie der Hyperparameter erstellen damit die werte anschließend wieder zurückgesetzt werden\n",
    "hParamBase = hyperparameter.copy()\n",
    "\n",
    "    \n",
    "for i in range(len(hyperparameterSet)):   \n",
    "    for j in range(len(hyperparameterSet[i])):\n",
    "        print(intToHyperparameter[i])\n",
    "        hyperparameter[intToHyperparameter[i]] = hyperparameterSet[i][j]\n",
    "        print(hyperparameter)\n",
    "\n",
    "        for k in range(setTests):\n",
    "            print(k)\n",
    "            # die Timeout Time bestimmt nach wieviel Minuten ein Run abbricht\n",
    "            train(info = False, numEpisodes = hyperparameter[\"Run_NumEpisoden\"], auto = auto, timeoutTime = 30)\n",
    "     \n",
    "    hyperparameter[intToHyperparameter[i]] = hParamBase[intToHyperparameter[i]]\n",
    "    clear_output(wait=True)\n",
    "            \n",
    "        \n",
    "for k in range(0,kombinationTraining):\n",
    "    chooseRandomHyperparameter = random.randint(0, len(hyperparameterSet)-1)\n",
    "    \n",
    "    sample = random.sample(hyperparameterSet[chooseRandomHyperparameter], k=1) \n",
    "        \n",
    "    hyperparameter[intToHyperparameter[chooseRandomHyperparameter]] = sample[0]\n",
    "\n",
    "    for l in range(setTests):\n",
    "        # die Timeout Time bestimmt nach wieviel Minuten ein Run abbricht\n",
    "        train(info = False, numEpisodes = hyperparameter[\"Run_NumEpisoden\"], auto = auto, timeoutTime = 30)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "     \n",
    "    hyperparameter = hParamBase\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df185d61",
   "metadata": {},
   "source": [
    "Trainieren mit einen Hyperparameter satz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(render = False, info = False, numEpisodes = hyperparameter[\"Run_NumEpisoden\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fdcfb",
   "metadata": {},
   "source": [
    "### Laden von einem Model\n",
    "Anhand von https://www.youtube.com/watch?v=9L9jEOwRrCg&feature=youtu.be 08.01.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da750f1",
   "metadata": {},
   "source": [
    "Das baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(render = True, info = False, numEpisodes = hyperparameter[\"Run_NumEpisoden\"], load = \"C:/Users/fabia/OneDrive/Semester3/ProjektSeminar/Projektseminar-DeepLearing-FabianFleischer/model/baselineModel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b0372",
   "metadata": {},
   "source": [
    "id 1673235165.4452035 <br>\n",
    "Unterschied zum baseline Model ist Reward_SameWayTwice = -120000 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = {\n",
    "                    \"Agent_MaxMemory\": 100000,\n",
    "                    \"Agent_BatchSize\": 250,\n",
    "                    \"Agent_LearningRate\": 0.00100,\n",
    "                    \"Agent_EpsilonStartValue\": 80, \n",
    "                    \"Agent_Gamma\": 0.9,\n",
    "                    \"Env_Memory\": 10000,\n",
    "                    \"Env_Epsilon\": 8000,\n",
    "                    \"Env_Size\": 7,\n",
    "                    \"Reward_SameWayTwice\": -120000,\n",
    "                    \"Reward_ClosedTarget\": 1000000,\n",
    "                    \"Reward_Terminated\": 1000000,\n",
    "                    \"Reward_RunAgainstWall\": -100000,\n",
    "                    \"Run_NumEpisoden\": 10,\n",
    "                 }\n",
    "\n",
    "train(render = True, info = False, numEpisodes = hyperparameter[\"Run_NumEpisoden\"], load = \"C:/Users/fabia/OneDrive/Semester3/ProjektSeminar/Projektseminar-DeepLearing-FabianFleischer/model/model_043426_09012023.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc2b16",
   "metadata": {},
   "source": [
    "## Auswertung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4779c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "data = pd.read_csv('C:/Users/fabia/OneDrive/Semester3/ProjektSeminar/testDataCSV/hparams_table.csv', sep=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27db8b0",
   "metadata": {},
   "source": [
    "Die Daten werden nach den Hyperparamerter Konstelationen Gruppiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "dataGbHypaparameter = data.groupby(['Agent_MaxMemory','Agent_BatchSize','Agent_LearningRate','Agent_EpsilonStartValue','Agent_Gamma','Env_Memory','Env_Epsilon','Reward_SameWayTwice','Reward_ClosedTarget','Reward_Terminated','Reward_RunAgainstWall'])\n",
    "\n",
    "dataGbHypaparameterAgg = dataGbHypaparameter.agg({\n",
    "                                                    'hparam/reward': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                    'hparam/steps': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                    'hparam/euklidischeDistanz': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                    'hparam/Episode': ['mean', 'max', 'std', 'var']\n",
    "                                                })\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(dataGbHypaparameterAgg.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5686d",
   "metadata": {},
   "source": [
    "Die Daten werden nach den Hyperparamerter Konstelationen und der Episode Gruppiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "dataGbHypaparameterEpisode = data.groupby(['Agent_MaxMemory','Agent_BatchSize','Agent_LearningRate','Agent_EpsilonStartValue','Agent_Gamma','Env_Memory','Env_Epsilon','Reward_SameWayTwice','Reward_ClosedTarget','Reward_Terminated','Reward_RunAgainstWall','hparam/Episode'])\n",
    "\n",
    "dataGbHypaparameterAggEpisode = dataGbHypaparameterEpisode.agg({\n",
    "                                                    'hparam/reward': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                    'hparam/steps': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                    'hparam/euklidischeDistanz': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                })\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(dataGbHypaparameterAggEpisode.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed64e2b",
   "metadata": {},
   "source": [
    "Datensatz laden mit nur den Testergebissen der 9. Episode da diese zeigen das der Agent erfolgreich den Testlauf zu durchlaufe ohne ein Timeout auszulösen. Zudem ist hier das Epsilon am weitesten abgeflacht un der Agent trifft kaum noch random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f070379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataEpisode9 = pd.read_csv('C:/Users/fabia/OneDrive/Semester3/ProjektSeminar/testDataCSV/hparams_table_Episode9.csv', sep=',')\n",
    "\n",
    "dataEpisode9GbHypaparameter = dataEpisode9.groupby(['Agent_MaxMemory','Agent_BatchSize','Agent_LearningRate','Agent_EpsilonStartValue','Agent_Gamma','Env_Memory','Env_Epsilon','Reward_SameWayTwice','Reward_ClosedTarget','Reward_Terminated','Reward_RunAgainstWall'])\n",
    "\n",
    "dataEpisode9GbHypaparameterAgg = dataEpisode9GbHypaparameter.agg({\n",
    "                                                    'hparam/reward': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                    'hparam/steps': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                    'hparam/euklidischeDistanz': ['mean', 'min', 'max', 'std', 'var'],\n",
    "                                                })\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(dataEpisode9GbHypaparameterAgg.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968300d",
   "metadata": {},
   "source": [
    "Im volgenden werden die gesamten Resultate all der Kombinationen verglichen die, die 9. Episode erreicht haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4962ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = dataEpisode9[['Agent_MaxMemory','Agent_BatchSize','Agent_LearningRate','Agent_EpsilonStartValue','Agent_Gamma','Env_Memory','Env_Epsilon','Reward_SameWayTwice','Reward_ClosedTarget','Reward_Terminated','Reward_RunAgainstWall']].to_numpy()\n",
    "steps = dataEpisode9['hparam/steps'].to_numpy()\n",
    "reward = dataEpisode9['hparam/reward'].to_numpy()\n",
    "euklidischeDistanz = dataEpisode9['hparam/euklidischeDistanz'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16a528",
   "metadata": {},
   "source": [
    "Hilfsdictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intToHyperparameter={\n",
    "                        0:\"Agent_MaxMemory\",\n",
    "                        1:\"Agent_BatchSize\",\n",
    "                        2:\"Agent_LearningRate\",\n",
    "                        3:\"Agent_EpsilonStartValue\", \n",
    "                        4:\"Agent_Gamma\",\n",
    "                        5:\"Env_Memory\",\n",
    "                        6:\"Env_Epsilon\",\n",
    "                        7:\"Reward_SameWayTwice\",\n",
    "                        8:\"Reward_ClosedTarget\",\n",
    "                        9:\"Reward_Terminated\",\n",
    "                        10:\"Reward_RunAgainstWall\",\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f90553",
   "metadata": {},
   "source": [
    "Auswertung der anzahl der Steps in Episode 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419047be",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSteps = max(steps)\n",
    "minSteps = min(steps)\n",
    "meanSteps = steps.mean()\n",
    "varSteps = np.var(steps)\n",
    "stdSteps = np.std(steps, ddof=1)\n",
    "\n",
    "print('maxSteps:', maxSteps, 'minSteps:', minSteps, 'meanSteps:', meanSteps, 'varSteps:', varSteps, 'stdSteps:', stdSteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de119aeb",
   "metadata": {},
   "source": [
    "Hyperparametersatz der die wenigsten steps in der 9. Episode benötigt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(steps == minSteps)[0][0]\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(intToHyperparameter[i], '=', hparam[index][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06503a52",
   "metadata": {},
   "source": [
    "Auswertung des Reward in der 9. Episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxReward = max(reward)\n",
    "minReward = min(reward)\n",
    "meanReward = reward.mean()\n",
    "varReward = np.var(reward)\n",
    "stdReward = np.std(reward, ddof=1)\n",
    "\n",
    "print('maxReward:', maxReward, 'minReward:', minReward, 'meanReward:', meanReward, 'varReward:', varReward, 'stdReward:', stdReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811b095",
   "metadata": {},
   "source": [
    "Hyperparametersatz mit dem höchsten reward in der 9. Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(reward == maxReward)[0][0]\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(intToHyperparameter[i], '=', hparam[index][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafa169",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEuklidischeDistanz = max(euklidischeDistanz)\n",
    "minEuklidischeDistanz = min(euklidischeDistanz)\n",
    "meanEuklidischeDistanz = euklidischeDistanz.mean()\n",
    "varEuklidischeDistanz = np.var(euklidischeDistanz)\n",
    "stdEuklidischeDistanz = np.std(euklidischeDistanz, ddof=1)\n",
    "\n",
    "print('maxEuklidischeDistanz:', maxEuklidischeDistanz , 'minEuklidischeDistanz:', minEuklidischeDistanz , 'meanEuklidischeDistanz:', meanEuklidischeDistanz , 'varEuklidischeDistanz:', varEuklidischeDistanz , 'stdEuklidischeDistanz:', stdEuklidischeDistanz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(euklidischeDistanz == minEuklidischeDistanz)[0][0]\n",
    "print(euklidischeDistanz[index])\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(intToHyperparameter[i], '=', hparam[index][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9010e336",
   "metadata": {},
   "source": [
    "### Ergebniss\n",
    "Was hierbei ersichtlich wird ist das der ein Hyperparametersatz die geringsten Steps, den maximalen Rewrad und die geringste Euklidische Distanz erziehlte. Dieser Hyperparametersatz hat es in 3 verscheiden Testläufen in die 9. Episode geschafft. Was bei diesem Hyperprametersatz aufällig ist und sich insbesonder in den Basline Testdaten wiederspiegelt ist die hohe varianz der ergebnisse bei gleichem Hyperparametersatz. Hierdurch ist es schwer bis unmöglich aus den gegeben Testdaten tatsächlich die besten Hyperparameterwerte festzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45136c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "dataEpisode9GbHypaparameterAgg.reset_index().iloc[51]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4aa655",
   "metadata": {},
   "source": [
    "### Ausblick\n",
    "Testen mit weitaus mehreren testläufen pro Hyperparametersatz mit dem Ziel die Varianz der Resultate zu veringern und dadurch stcihhaltige aussagen über die performance eines Hyperparameters gegenüber der Baseline zu machen. <br><br>\n",
    "Den ersten Agent überarbeiten und mit ihn mit dem Zweiten agent zu vergelichen. <br><br>\n",
    "Eine AI mit den Hyperparameterdaten trainerum um diese die beste Hyperparameter konstellation finden zu lassen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d2566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea85e8f88a27266b11d8c0568e755a1d7c883016e0e987ddc6b2646fce510ba5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
